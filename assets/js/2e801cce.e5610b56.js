"use strict";(self.webpackChunkdocusaurus_blog_with_search=self.webpackChunkdocusaurus_blog_with_search||[]).push([[9450],{6029:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2025-02-23-deploy-rust-to-azure-container-app.md","metadata":{"permalink":"/2025-02-23-deploy-rust-to-azure-container-app.md","source":"@site/blog/2025-02-23-rust-template-azure-container-apps.md","title":"Deploy Rust app to Azure Container App","description":"Deploy a Hello World app written in Rust to Azure Container app","date":"2025-02-23T00:00:00.000Z","formattedDate":"February 23, 2025","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"Azure Container App","permalink":"/tags/azure-container-app"},{"label":"Rust","permalink":"/tags/rust"},{"label":"GitHub Action","permalink":"/tags/git-hub-action"}],"readingTime":1.865,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2025-02-23-deploy-rust-to-azure-container-app.md","canonical_url":"https://dfberry.github.io/blog/2025-02-23-deploy-rust-to-container-app.md","custom_edit_url":null,"sidebar_label":"2025-02-23 Deploy Rust to container app","title":"Deploy Rust app to Azure Container App","description":"Deploy a Hello World app written in Rust to Azure Container app","published":false,"tags":["Azure","Azure Container App","Rust","GitHub Action"],"updated":"2025-02-23 00:00 PST"},"nextItem":{"title":"Deploy an Azure Functions app from a monorepo with a GitHub Action for Node.js","permalink":"/2024-04-07-mono-repo-functions.md"}},"content":"If you\'re a Rust developer who\'s just getting started with Azure, this post is for you. In this guide, I\'ll explore an templated repository you can use that makes deploying your Rust application to Azure Container Apps a breeze. With easy-to-use CLI tools and an automated GitHub Action workflow, deploying your app is as simple as pushing your changes to the main branch!\\n\\n## What\'s in the Repo?\\n\\nThis repository contains a simple Rust application built using the Axum web server. It includes:\\n\\n- A root route that returns HTML.\\n- An `/api/users` endpoint returning JSON data.\\n- Example configuration for environment secrets:\\n  - `PORT=3000`\\n  - `MY_SECRET=1234`\\n\\n## Easy, CLI-Powered provisioning and deployment\\n\\n### 1. Provision Your Azure Environment\\nThe repo ships with a set of bash scripts that take care of provisioning Azure resources. These scripts leverage:\\n\\n- **Azure CLI** \u2013 Use it to interact with and manage your Azure resources.\\n- **Azure Developer CLI (azd)** \u2013 Provision and configure resources using Bicep templates.\\n- **GitHub CLI** \u2013 Configure the deployment secrets necessary for GitHub Actions.\\n\\nRunning these scripts sets up everything from an Azure Container Registry to Application Insights in just a few commands.\\n\\n### 2. Push-to-Main Deployments\\nAfter setting up your environment, subsequent deployments are incredibly simple:\\n\\n- **Automated Workflow:** Every push to the `main` branch triggers a GitHub Action that builds your Rust application and deploys it to Azure Container Apps.\\n- **Continuous Deployment:** With the automation in place, you no longer need to manually deploy your app. Just push your changes\u2014and your app is live!\\n\\n### 3. Real-Time Monitoring and Logs\\nThe repository includes a script (`stream-logs.sh`) that streams live logs from your Rust Axum server directly to your terminal. This makes it easy to:\\n\\n- Monitor live application activity.\\n- Debug issues in real time.\\n- Gain immediate insights into your app\u2019s behavior.\\n\\n## Simple Rust migration to Azure\\n\\nFor developers new to Azure but familiar with Rust, this repository exemplifies how modern dev tools combine to simplify cloud deployments. Instead of wrestling with complex setups, you get:\\n\\n- A straightforward CLI-driven process.\\n- Seamless integration with GitHub Actions.\\n- An efficient, automated deployment pipeline that gets your app live with minimal hassle.\\n\\n## Repository template\\n\\n[https://github.com/dfberry/rust-on-azure-container-apps](https://github.com/dfberry/rust-on-azure-container-apps)"},{"id":"/2024-04-07-mono-repo-functions.md","metadata":{"permalink":"/2024-04-07-mono-repo-functions.md","source":"@site/blog/2024-04-07-mono-repo-functions.md","title":"Deploy an Azure Functions app from a monorepo with a GitHub Action for Node.js","description":"Monorepos for Node.js are a unique challenge with Azure functions. Learn 2 tricks to successfully deploy.","date":"2024-04-07T00:00:00.000Z","formattedDate":"April 7, 2024","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"Azure Functions","permalink":"/tags/azure-functions"},{"label":"Developer Experience","permalink":"/tags/developer-experience"},{"label":"Deploy","permalink":"/tags/deploy"}],"readingTime":3.815,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2024-04-07-mono-repo-functions.md","canonical_url":"https://dfberry.github.io/blog/2024-04-07-mono-repo-functions.md","custom_edit_url":null,"sidebar_label":"2024-04-07 Azure Functions in mono repos","title":"Deploy an Azure Functions app from a monorepo with a GitHub Action for Node.js","description":"Monorepos for Node.js are a unique challenge with Azure functions. Learn 2 tricks to successfully deploy.","published":false,"tags":["Azure","Azure Functions","Developer Experience","Deploy"],"updated":"2024-04-07 00:00 PST"},"prevItem":{"title":"Deploy Rust app to Azure Container App","permalink":"/2025-02-23-deploy-rust-to-azure-container-app.md"},"nextItem":{"title":"Examples of code includes for Dev.to","permalink":"/2024-04-06-code-includes.md"}},"content":"Azure Functions apps can be locally deployed from Visual Studio Code using the Azure Functions extension or when you create the resource in the portal, you can configure deployment. These are straightforward when your app is the only thing in the repo but become a little more challenging in monorepos. \\n\\n## Single versus monorepo repositories\\n\\nWhen you have a single function in a repo, the Azure Functions app is build and run from the root level package.json which is where hosting platforms look for those files. \\n\\n```text\\n- package.json\\n- package-lock.json\\n- src\\n    - functions\\n        - hello-world.js\\n```\\n\\nIn a monorepos, all these files are pushed down a level or two and there may or may not be a root-level package.json.\\n\\n```text\\n- package.json\\n- packages\\n    - products\\n        - package.json\\n        - package-lock.json\\n        - src\\n            - functions\\n                - product.js\\n    - sales\\n        - package.json\\n        - package-lock.json\\n        - src\\n            - functions\\n                - sales.js\\n```\\n\\nIf there is a root-level package.json, it may control developer tooling across all packages. While you can deploy the entire repo to a hosting platform and configure which package is launched, this isn\'t necessary and may lead to problems. \\n\\n## Monorepo repositories as a single source of truth\\n\\nMonorepo repositories allow you to collect all source code or at least all source code for a project into a single place. This is ideal for microservices or full-stack apps. There is an extra layer of team education and repository management in order to efficiently operationalize this type of repository. \\n\\nWhen starting the monorepo, you need to select the workspace management. I use npm workspaces but others exist. This requires a root-level package.json with the packages (source code projects) noted.\\n\\nThe syntax for npm workspaces allows you to select what is a package as well as what is not a package. \\n\\n```json reference\\nhttps://github.com/dfberry/dfberry-blog/blob/main/snippets/2024-04-07-functions-monorepo/package-workspaces.json\\n```\\n\\n## Azure Functions apps with Visual Studio Code\\n\\nWhen you create a Functions app with Visual Studio Code with the Azure Functions extension you can select it to be created at the root, or in a package. As part of that creation process, a `.vscode` folder is created with files to help find and debug the app. \\n\\n* extensions.json: all Visual Studio Code extensions\\n* launch.json: debug\\n* settings.json: settings for extensions\\n* tasks.json: tasks for launch.json\\n\\nThe **settings.json** includes `azureFunctions.deploySubpath` and `azureFunctions.projectSubpath` properties which tells Azure Functions where to find the source code. For a monorepo, the value of these settings may depend on the version of the extension you use. \\n\\nAs of March 2024, setting the exact path has worked for me, such as `packages/sales/`. \\n\\nIf you don\'t set the correct path for these values, the correct package may not be used with the extension or the hosting platform won\'t find the correct package.json to launch the Node.js Functions app. \\n\\n* During development: set the `azureFunctions.projectSubpath` to the single package path you are developing.\\n* During deployment: set the `azureFunctions.deploySubpath` to the single package path so the hosting platform has the correct path to launch the app. \\n\\n## GitHub actions workflow file for Azure Functions monorepo app\\n\\nWhen you create a Azure Functions app in the Azure portal and configure the deployment, the default (and not editable) workflow file is built for a **monorepo** where the app\'s package.json is at the root of the repository.\\n\\n## Yaml\\n\\n```yml reference\\nhttps://github.com/dfberry/dfberry-blog/blob/main/snippets/2024-04-07-functions-monorepo/single-app-workflow.yml\\n```\\n\\nThis worklow sets the `AZURE_FUNCTIONAPP_PACKAGE_PATH` as the root of the project then pushes, `pushd \'./${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}\'`, into that path to build. The zip, `zip release.zip ./* -r`, packages up everything as the root. To use a monorepo, these need to be altered. \\n\\n1. Change the name of the workflow to indicate the package and project.\\n\\n    ```yml\\n    name: Build & deploy Azure Function - sales\\n    ```\\n\\n1. Create a new global `env` parameter that sets the package location for the subdirectory source code. \\n\\n    ```yml\\n    PACKAGE_PATH: \'packages/sales\' \\n    ```\\n\\n1. Change the `Resolve Project Dependencies Using Npm` to include the new environment variable.\\n\\n    ```yml\\n    pushd \'./${{ env.AZURE_FUNCTIONAPP_PACKAGE_PATH }}/${{ PACKAGE_PATH }}\'\\n    ```\\n\\n    The `pushd` commands moves the context into that sales subdirectory.\\n\\n1. Change the `Zip artifact for deployment` to use `pushd` and `popd` and include the new environment variable. The `popd` command returns the context to the root of the project. \\n\\n    Using the `pushd` command, change the location of the generated zip file to be in root directory. \\n\\n    The result is that the zip file\'s file structure looks like:\\n    \\n    ```text\\n    - package.json\\n    - src\\n        - functions\\n            - sales.js\\n\\n    ```\\n\\n1. The final workflow file for a monorepo repository with an Azure functions package is:\\n\\n```yml reference\\nhttps://github.com/dfberry/dfberry-blog/blob/main/snippets/2024-04-07-functions-monorepo/mono-app-workflow.yml\\n```"},{"id":"/2024-04-06-code-includes.md","metadata":{"permalink":"/2024-04-06-code-includes.md","source":"@site/blog/2024-04-06-remote-code.md","title":"Examples of code includes for Dev.to","description":"How to add code includes for Dev.to","date":"2024-04-06T00:00:00.000Z","formattedDate":"April 6, 2024","tags":[],"readingTime":0.76,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2024-04-06-code-includes.md","canonical_url":"https://dfberry.github.io/blog/2024-04-06-code-includes.md","custom_edit_url":null,"sidebar_label":"2024-04-06 Code includes for Dev.to","title":"Examples of code includes for Dev.to","description":"How to add code includes for Dev.to","published":false},"prevItem":{"title":"Deploy an Azure Functions app from a monorepo with a GitHub Action for Node.js","permalink":"/2024-04-07-mono-repo-functions.md"},"nextItem":{"title":"Add a database service to the API project","permalink":"/2024-01-21-cloud-native-008-api-database.md"}},"content":"My GitHub site is built with Docusaurus 2 and usually shows code snippets. Since I build to my GitHub site but want to repost to Dev.to and other platforms from RSS feeds, I need the code snippets to work with the minimal amount of manual intervention and corrections after I import the blog post into Dev.to. \\n\\nIf the code snippet is inside the blog post as text, the Dev.to rss feed import mangles the code snippet syntax, which requires manual fix up which is time-consuming and a step I have to remember. \\n\\n## Using @saucelabs/theme-github-codeblock  \\n\\nI found the saucelabs code block on the [unofficial Docusaurus features](https://docusaurus.io/community/resources#features).\\n\\n## JavaScript reference: `js`\\n\\n```js reference\\nhttps://github.com/dfberry/mslearn-advocates.azure-functions-and-signalr/blob/main/client-end/src/index.js\\n```\\n\\n## Json reference: `json`\\n\\n```json reference\\nhttps://github.com/saucelabs/docusaurus-theme-github-codeblock/blob/main/package.json\\n```\\n\\n## Bash reference: `bash`\\n\\n```bash reference\\nhttps://github.com/ruanyf/simple-bash-scripts/blob/master/scripts/addition.sh\\n```\\n\\n\\n## Text reference: `text`\\n\\n```text reference\\nhttps://github.com/dfberry/dev-diary/blob/main/notes/Learning-DataLake/Neal.txt\\n```\\n\\n## Rust: reference: `rs`\\n\\n```rs reference\\nhttps://github.com/dfberry/rust-axum-mongodb-todo/blob/main/src/main.rs\\n```\\n\\n## Yaml\\n\\n```yml reference\\nhttps://github.com/dfberry/actions/blob/main/examples/mono-app-workflow.yml\\n```"},{"id":"/2024-01-21-cloud-native-008-api-database.md","metadata":{"permalink":"/2024-01-21-cloud-native-008-api-database.md","source":"@site/blog/2024-01-21-cloud-native-008-api-database.md","title":"Add a database service to the API project","description":"Add a data layer with the mongoose client to handle create, read, update, and delete operations for the API.","date":"2024-01-21T00:00:00.000Z","formattedDate":"January 21, 2024","tags":[{"label":"Cloud-native","permalink":"/tags/cloud-native"},{"label":"Database","permalink":"/tags/database"},{"label":"MongoDB","permalink":"/tags/mongo-db"},{"label":"Mongoose","permalink":"/tags/mongoose"}],"readingTime":11.145,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2024-01-21-cloud-native-008-api-database.md","canonical_url":"https://dfberry.github.io/blog/2024-01-21-cloud-native-008-api-database.md","custom_edit_url":null,"sidebar_label":"2024.01-21 Add MongoDB to project","title":"Add a database service to the API project","description":"Add a data layer with the mongoose client to handle create, read, update, and delete operations for the API.","published":false,"tags":["Cloud-native","Database","MongoDB","Mongoose"],"updated":"2024-01-21 00:00 PST"},"prevItem":{"title":"Examples of code includes for Dev.to","permalink":"/2024-04-06-code-includes.md"},"nextItem":{"title":"React Vite Client UI: quick Todo app proof of concept","permalink":"/2023-12-28-cloud-native-006-client-todo.md"}},"content":"The goal of **iteration 008** is to add a database to the project. \\n\\n* [Cloud-native-todo](https://github.com/dfberry/cloud-native-todo) GitHub repo\\n\\nAt this point, it doesn\'t matter if it is no-SQL, or SQL-based because there are no relationships or transactions. Someone suggested I look at Prisma for my ORM. After some testing, I realized a free Azure SQL database wasn\'t going to work because, Prisma requires a 2nd database, although only temporarily, for diffing the migrations. \\n\\nWhile I\'m sure Prisma has its purpose, at this stage of the project, it seems like overkill compared to adding a database and client library I\'m more familiar with. A point in the project where boring is good. \\n\\nSince I\'m already on Azure, selecting some flavor of SQL Server or Cosmos DB makes sense if there is a consumption (pay-as-you-go) pricing tier (SKU) which is free-ish for such as small project. Mongoose and the Cosmos DB API for MongoDB are expedient choices given the wealth of documentation for both for TypeScript/JavaScript. \\n\\n* [Mongo DB developer guide for Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/how-to-javascript-get-started?tabs=azure-cli%2Clinux)\\n* [Mongoose documentation](https://mongoosejs.com/docs/guide.html)\\n\\n## Add a MongoDB container to the development environment\\n\\nAll the local services are managed by Docker compose for local development where possible. Add the MongoDB container so development and testing don\'t incur any pay-as-you-go costs. \\n\\n```yaml\\nversion: \\"3\\"\\n\\nservices:\\n  api-todo:\\n    build:\\n      context: ./api-todo\\n    ports:\\n      - \\"3000:3000\\"\\n    depends_on:\\n      - mongodb\\n\\n  client-todo:\\n    build:\\n      context: ./client-todo\\n    environment:\\n      VITE_USE_LOCAL_API: \\"true\\"\\n      VITE_API_URL: http://localhost:3000\\n    ports:\\n      - \\"80:80\\"\\n    depends_on:\\n      - api-todo\\n\\n  mongodb:\\n    image: mongo:5.0\\n    restart: always\\n    environment:\\n      - MONGO_INITDB_ROOT_USERNAME=mongo\\n      - MONGO_INITDB_ROOT_PASSWORD=MongoPass\\n    ports:\\n      - \\"27017:27017\\"\\n    volumes:\\n      - ./mongodata:/data/db\\n\\nvolumes:\\n  mongodata:\\n```\\n\\nStart the service in a separate terminal with:\\n\\n```bash\\ndocker compose up mongodb\\n```\\n\\nI stole this idea from the [Contoso Real Estate](https://github.com/azure-samples/contoso-real-estate) project which has a wealth of development environment configuration for you to use. \\n\\nNow that the database is running, add the MongoDB viewer.\\n\\n## Visual Studio Code extension for MongoDB\\n\\nMake sure you add the [MongoDB](https://marketplace.visualstudio.com/items?itemName=mongodb.mongodb-vscode) viewer extension to the development environment, in the `devcontainer.json`. \\n\\n```json\\n\\t\\"customizations\\": {\\n\\t\\t\\"vscode\\": {\\n\\t\\t\\t\\"extensions\\": [\\n\\t\\t\\t\\t...other extensions,\\n\\n\\t\\t\\t\\t\\"mongodb.mongodb-vscode\\",\\n\\t\\t\\t]\\n\\t\\t}\\n\\t},\\n```\\n\\nYou can add a connection with a connection string so this can be used for both local and cloud databases. \\n\\n\\n## TODO shape\\n\\nThe shape of the TODO prior to this iteration was: \\n\\n```json\\n{\\n  id: 123\\n  title: \'Get Milk\'\\n}\\n```\\n\\nUpdate the shape to allow for data shape growth:\\n\\n```json\\n{\\n  id: \'65ad9ad0769c2853d2804f3f\',\\n  title: \'Get Milk\',\\n  description: \'the oaty kind\',\\n  createdAt: \'2024-01-21T22:29:36.849Z\',\\n  updatedAt: \'\'\\n}\\n```\\n\\nThe title and description should have a max size to help the UI. \\n\\n## Install mongoose to API\\n\\nTypeScript types are already in the package so just install it. \\n\\n```bash\\nnpm install mongoose\\n```\\n\\nThe `package.json` shows `\\"mongoose\\": \\"^8.0.4\\",` in the dependencies property. \\n\\n## Connect to the database\\n\\nBefore jumping in with code in the API, make sure you can connect to the database with the client library. Design your schema and make sure any restrictions, validations, and transformations are complete. Leave the script in the repo, it will be handy for the next person onboarded to the project to not have to figure out how to connection and view data. Keep this connection script all as a single file. This allows someone new to the team and Mongoose to understand how the pieces fit together. \\n\\n```javascript\\nconst mongoose = require(\'mongoose\');\\nconst Schema = mongoose.Schema;\\n\\nconst COLLECTION = \'TodoConnectionTest\';\\n\\n// Run mongo with `docker compose up mongodb`\\nconst URI = \'mongodb://mongo:MongoPass@localhost:27017/\';\\n\\nconst TodoSchema = new Schema(\\n  {\\n    title: {\\n      type: String,\\n      unique: true,\\n      minlength: 1,\\n      maxlength: 40,\\n    },\\n    description: {\\n      type: String,\\n      maxlength: 1000,\\n      default: null,\\n    },\\n    createdAt: {\\n      type: String,\\n    },\\n    updatedAt: {\\n      type: String,\\n      default: null,\\n    },\\n  },\\n  {\\n    versionKey: false,\\n    virtuals: true,\\n  }\\n);\\n\\nTodoSchema.virtual(\'id\').get(function () {\\n  return this._id.toHexString();\\n});\\n\\n// Ensure virtual fields are serialised.\\nTodoSchema.set(\'toJSON\', {\\n  virtuals: true,\\n  versionKey: false,\\n  transform: function (doc, ret) {\\n    delete ret._id;\\n  },\\n});\\n\\nconst main = async () => {\\n  // Connect to db\\n  mongoose.connect(URI);\\n\\n  // Create a model\\n  const TodoDb = mongoose.model(COLLECTION, TodoSchema);\\n\\n  // Using create\\n  const saveResult1 = await TodoDb.create({\\n    title: \'first todo\',\\n    description: \'description\',\\n    createdAt: new Date().toISOString(),\\n  });\\n  const transformed1 = saveResult1.toJSON();\\n  console.log(\'Created lean--------------------------------\');\\n  console.log(transformed1);\\n\\n  // ADD MORE COMMANDS\\n\\n};\\n\\nmain()\\n  .then(() => {\\n    console.log(\'done\');\\n    mongoose.disconnect();\\n  })\\n  .catch((e) => {\\n    console.log(e);\\n  });\\n```\\n\\nAdd a script to the `package.json` so you can test the connection:\\n\\n```json\\n\\"mongoose:test\\": \\"node ./scripts/test-mongo-connection.js\\"\\n```\\n\\n## TypeScript database service\\n\\nStart with a generic CRUD class. All MongoDB collections will use this class to enforce consistency. \\n\\n```typescript\\nexport default class CrudService<T> {\\n  #model: Model<T>;\\n\\n  constructor(model: Model<T>) {\\n    this.#model = model;\\n  }\\n\\n  // Add\\n  async add(doc: Partial<T>): Promise<T> {\\n      const improvedDoc = {\\n        ...doc,\\n        createdAt: new Date().toISOString(),\\n        updatedAt: null,\\n      };\\n      const data = await this.#model.create(improvedDoc);\\n\\n      return data?.toJSON();\\n  }\\n\\n  // Read\\n  async get(id: string): Promise<T> {\\n\\n      const data = await this.#model.findById(id);\\n\\n      return data: data?.toJSON();\\n  }\\n\\n  // Update\\n  async update(\\n    id: string,\\n    update: Partial<T>\\n  ): Promise<T> {\\n      const improvedDoc = { ...update, updatedAt: new Date().toISOString() };\\n\\n      const data = await this.#model.findByIdAndUpdate(id, improvedDoc, {\\n        new: true,\\n      });\\n\\n      return data?.toJSON();\\n  }\\n\\n  // Delete\\n  async delete(id: string): Promise<T> {\\n      const data = await this.#model.findByIdAndDelete(id);\\n\\n      return data?.toJSON();\\n  }\\n\\n  // Get All\\n  async getAll(): Promise<T[]> {\\n      const data = await this.#model.find();\\n      return data;\\n  }\\n\\n  // Delete All\\n  async deleteAll(): Promise<unknown> {\\n      const deleteAllResponse = await this.#model.deleteMany({});\\n      return deleteAllResponse;\\n  }\\n\\n  // Batch insert\\n  async seed(docs: T[] | Partial<T>[]): Promise<T[]> {\\n      const result = await this.#model.insertMany(docs);\\n      return data;\\n  }\\n}\\n```\\n\\nMongoDB and the Mongoose client provide a high degree of configuration for what type of information to return from the mongoose calls. Its important to play with this in the previous script to determine what you want returned then apply those changes to this Crud class and the schema via the model that it uses. \\n\\n* **`_id` versus `id`**: MongoDB stores the unique id as `_id` but I want the REST API and the UI to only use `id`. Any transformations need to be done at this data layer. If this data service was used for automation or other movement of data between backend services, that would probably require some strict contracts so an ambitious DBA didn\'t make assumptions that the native `_id` was required. \\n* transformation on **single versus multiple** items: many of the convenience functions run a query inside the mongoose client which is meant to operate on multiple values. When running queries, transformations applied to a single object (such as with create()) aren\'t applied to the objects. You need to either transform the objects yourself, or provide an aggregation pipeline to make sure you get the shape returned which you expect. This means your tests need to validate the shape of objects for all CRUD operations where you want data returned. You may opt to have have the transformations applied at the CRUD class level and the schema level, if the owner of the application code and the owner of the schema object definition are different people. For example, the tests might include:\\n  * Test property property count\\n  * Test property names\\n  * Test that `_id` and `_v` aren\'t returned\\n  * Test a new item only has the createdAt date\\n  * Test an updated item only has the updatedAt data\\n* **data returned**: the mongoose client methods can return a stunning variety of values and information. For example, when updating, the returned information can include the data set in, the data after it was updated, or include the number of items which were updated. Be clear in your design when to return what kind of information. The API layer should only return what the UI needs. \\n\\n## Use the CRUD class for collections\\n\\nCreate an interface to provide a data layer contract:\\n\\n```typescript\\nexport interface IDataClass<T> {\\n  add: (todo: Partial<T>) => Promise<T>;\\n  get: (id: string) => Promise<T>;\\n  getAll: () => Promise<T[]>;\\n  update: (id: string, todo: Partial<T>) => Promise<T>;\\n  delete: (id: string) => Promise<T>;\\n  deleteAll: () => Promise<unknown>;\\n  batch: (todos: T[]) => Promise<T[]>;\\n}\\n```\\n\\nIf there are specific validations or transformations for a collection, apply them at a layer above the generic CRUD class. \\n\\n```typescript\\nexport type CrudServiceResponse<T> = {\\n  data: T | T[] | unknown | null;\\n  error: Error | null | ValidationError | ValidationError[] | undefined;\\n  valid?: boolean;\\n};\\n\\nexport class TodoService implements IDataClass<Todo> {\\n  #service: CrudService<Todo>;\\n\\n  constructor(connection: mongoose.Connection) {\\n    const ConnectedTodoModel = connection.model<Todo>(\'Todo\', TodoSchema);\\n    this.#service = new CrudService<Todo>(ConnectedTodoModel);\\n  }\\n\\n  async get(id: string): Promise<CrudServiceResponse<Todo>> {\\n    if (!id) {\\n      return { data: null, error: new Error(\'id is required\') };\\n    }\\n\\n    return await this.#service.get(id);\\n  }\\n\\n  async add(todo: Partial<Todo>): Promise<CrudServiceResponse<Todo>> {\\n    const { valid, error } = isValidPartial(todo);\\n    if (!valid) {\\n      return { data: null, error: error };\\n    }\\n    const addResponse = await this.#service.add(todo);\\n    return addResponse;\\n  }\\n\\n  async update(\\n    id: string,\\n    todo: Partial<Todo>\\n  ): Promise<CrudServiceResponse<Todo>> {\\n    if (!id) {\\n      return { data: null, error: new Error(\'id is required\') };\\n    }\\n\\n    const { valid, error } = isValidPartial(todo);\\n    if (!valid) {\\n      return { data: null, error: error };\\n    }\\n\\n    const updateResponse = await this.#service.update(id, {\\n      title: todo.title as string,\\n      description: todo.description as string,\\n      updatedAt: new Date().toISOString(),\\n    } as Todo);\\n    return updateResponse;\\n  }\\n\\n  async delete(id: string): Promise<CrudServiceResponse<Todo>> {\\n    if (!id) {\\n      return { data: null, error: new Error(\'id is required\') };\\n    }\\n\\n    return await this.#service.delete(id);\\n  }\\n  async getAll(): Promise<CrudServiceResponse<Todo[]>> {\\n    return await this.#service.getAll();\\n  }\\n  async seed(\\n    incomingTodos: Partial<Todo>[]\\n  ): Promise<CrudServiceResponse<Todo[]>> {\\n    return await this.#service.seed(incomingTodos);\\n  }\\n  async deleteAll(): Promise<CrudServiceResponse<Todo[]>> {\\n    const deleteResponse = await this.#service.deleteAll();\\n    return deleteResponse;\\n  }\\n}\\n```\\n\\n## Create the API routes and handlers\\n\\nThe API is separated between individual and multiple items. \\n\\n```typescript\\n// Multiples Routes\\n\\n// Create Todo router with all routes then export it\\nconst todosRouter = express.Router();\\n\\ntodosRouter.get(\'/\', getAllTodosHandler);\\ntodosRouter.patch(\'/\', batchUploadTodoHandler);\\ntodosRouter.delete(\'/\', deleteAllTodoHandler);\\n\\n// Catch-all route\\ntodosRouter.all(\'*\', (req, res) => {\\n  sendResponse(req, res, StatusCodes.NOT_FOUND, { error: \'Not Found\' });\\n  return;\\n});\\ntodosRouter.use(handleError);\\n```\\n\\n```typescript\\n// Singles Routes\\n// Create Todo router with all routes then export it\\nconst todoRouter = express.Router();\\n\\ntodoRouter.get(\'/:id\', getTodoHandler);\\ntodoRouter.post(\'/\', addTodoHandler);\\ntodoRouter.put(\'/:id\', updateTodoHandler);\\ntodoRouter.delete(\'/:id\', deleteTodoHandler);\\n\\n// Catch-all route\\ntodoRouter.all(\'*\', (req, res) => {\\n  sendResponse(req, res, StatusCodes.NOT_FOUND, { error: \'Not Found\' });\\n  return;\\n});\\ntodoRouter.use(handleError);\\n```\\n\\nPull in the routes to the Express app: \\n\\n```typescript\\n// Route that operates on a single todo\\napp.use(\'/todo\', todoRouter);\\n\\n// Route that operates on multiple todos\\napp.use(\'/todos\', todosRouter);\\n```\\n\\n## Test the APIs\\n\\nYou can use cURL, Postman, or Supertest. \\n\\n```bash\\n## Single\\ncurl -X GET http://localhost:3000/todo/65ac3b70d3adb5df333004d7 --verbose\\ncurl -X POST -H \\"Content-Type: application/json\\" -d \'{\\"todo\\": {\\"title\\":\\"CURL New Todo\\", \\"description\\":\\"This is a new todo\\"}}\' http://localhost:3000/todo --verbose\\ncurl -X PUT -H \\"Content-Type: application/json\\" -d \'{\\"todo\\": {\\"title\\":\\"CURL XXX Updated Todo\\", \\"description\\":\\"This is an updated todo\\"}}\' http://localhost:3000/todo/65ac3d1b4c60586e545b3628 --verbose\\ncurl -X DELETE http://localhost:3000/todo/65ac396a9afd90f786ab1fee --verbose\\n\\n## Multiple\\ncurl -X GET http://localhost:3000/todos --verbose\\ncurl -X PATCH -H \\"Content-Type: application/json\\" -d @batch.json http://localhost:3000/todos/ --verbose\\ncurl -X DELETE http://localhost:3000/todos --verbose\\n```\\n\\n```typescript\\nimport request from \'supertest\';\\nimport configureApp from \'./server\'; // Import your Express app\\nimport \'dotenv/config\';\\n\\ndescribe(\'Todo API against running MongoDB\', () => {\\n  it(\'test all todo routes\', async () => {\\n    process.env.NODE_ENV = \'test\';\\n\\n    const { app, connection } = await configureApp();\\n    await request(app).delete(\'/todos\');\\n\\n    // Add one\\n    const addOneResponse = await request(app)\\n      .post(\'/todo\')\\n      .send({\\n        todo: {\\n          title: \'Sa1 - \' + Date.now(),\\n          description: \'Sa2 - \' + Date.now(),\\n        },\\n      });\\n    testAdd(addOneResponse);\\n\\n    // // Update one\\n    const updateOneResponse = await request(app)\\n      .put(\'/todo/\' + addOneResponse.body.data.id)\\n      .send({\\n        todo: {\\n          title: \'Su1 - \' + Date.now(),\\n          description: \'su2 \' + Date.now(),\\n        },\\n      });\\n    testUpdate(updateOneResponse);\\n\\n    // // Delete `Sa1`, `Su1` should still be there\\n    const deletedOneResponse = await request(app).delete(\\n      \'/todo/\' + addOneResponse.body.data.id\\n    );\\n    testDelete(deletedOneResponse);\\n\\n    // Batch all - after this call 3 items should be in the database\\n    // 3 B\\n    const addThreeBody = {\\n      todos: [\\n        {\\n          title: \'B1a \' + Date.now(),\\n          description: \'B1b\' + Date.now(),\\n        },\\n        {\\n          title: \'B2a\' + Date.now(),\\n          description: \'B2b\' + Date.now(),\\n        },\\n        {\\n          title: \'B3a\' + Date.now(),\\n          description: \'B3b\' + Date.now(),\\n        },\\n      ],\\n    };\\n    const batchResponse = await request(app).patch(\'/todos\').send(addThreeBody);\\n    testBatch(batchResponse);\\n\\n    // // Get All - should return four items\\n    const getAllResponse = await request(app).get(\'/todos\');\\n    testGetAll(getAllResponse, 3);\\n\\n    // Delete All\\n    const deleteAllResponse = await request(app).delete(\'/todos\');\\n    testDeleteAll(deleteAllResponse, 3);\\n\\n    if (connection) {\\n      connection.close();\\n    }\\n  }, 30000);\\n});\\n```\\n\\nMake sure you validate the data returned:\\n\\n```typescript\\n//write a function to test the shape of a Todo\\nconst testTodoShape = (todo) => {\\n  const keys = Object.keys(todo);\\n\\n  expect(keys.length).toEqual(5);\\n  expect(keys).toContainEqual(\'id\');\\n  expect(keys).toContainEqual(\'title\');\\n  expect(keys).toContainEqual(\'description\');\\n  expect(keys).toContainEqual(\'createdAt\');\\n  expect(keys).toContainEqual(\'updatedAt\');\\n};\\nconst testTodoArrayShape = (todos) => {\\n  expect(todos).toBeInstanceOf(Array);\\n  todos.forEach(testTodoShape);\\n};\\n\\nconst testAdd = (addResponse) => {\\n  // operational error\\n  expect(addResponse.error).toEqual(false);\\n\\n  const { status, body } = addResponse;\\n  expect(status).toEqual(201);\\n  const { data, error } = body;\\n  expect(error).toEqual(null);\\n  expect(data).not.toEqual(null);\\n  testTodoShape(data);\\n};\\n\\nconst testUpdate = (updateResponse) => {\\n  // operational error\\n  expect(updateResponse.error).toEqual(false);\\n\\n  const { status, body } = updateResponse;\\n  expect(status).toEqual(202);\\n  const { data, error } = body;\\n  expect(error).toEqual(null);\\n  expect(data).not.toEqual(null);\\n  testTodoShape(data);\\n};\\n\\nconst testDelete = (deleteResponse) => {\\n  // operational error\\n  expect(deleteResponse.error).toEqual(false);\\n\\n  const { status, body } = deleteResponse;\\n  expect(status).toEqual(202);\\n  const { data, error } = body;\\n  expect(error).toEqual(null);\\n  expect(data).not.toEqual(null);\\n  testTodoShape(data);\\n};\\n\\nconst testBatch = (batchResponse) => {\\n  // operational error\\n  expect(batchResponse.error).toEqual(false);\\n\\n  const { status, body } = batchResponse;\\n  expect(status).toEqual(201);\\n  const { data, error } = body;\\n  expect(error).toEqual(null);\\n  expect(data).not.toEqual(null);\\n  testTodoArrayShape(data);\\n};\\n\\nconst testGetAll = (getAllResponse, dataLength) => {\\n  // operational error\\n  expect(getAllResponse.error).toEqual(false);\\n\\n  const { status, body } = getAllResponse;\\n  expect(status).toEqual(200);\\n  const { data, error } = body;\\n  expect(error).toEqual(null);\\n  expect(data).not.toEqual(null);\\n  expect(data.length).toEqual(dataLength);\\n  testTodoArrayShape(data);\\n};\\n\\nconst testDeleteAll = (deleteAllResponse, dataLength) => {\\n  // operational error\\n  expect(deleteAllResponse.error).toEqual(false);\\n\\n  const { status, body } = deleteAllResponse;\\n  expect(status).toEqual(202);\\n  const { data, error } = body;\\n  expect(error).toEqual(null);\\n  expect(data).not.toEqual(null);\\n  expect(data.deletedCount).toEqual(dataLength);\\n};\\n```\\n\\n## Next step\\n\\nThe next step is to add this functionality to the cloud environment."},{"id":"/2023-12-28-cloud-native-006-client-todo.md","metadata":{"permalink":"/2023-12-28-cloud-native-006-client-todo.md","source":"@site/blog/2023-12-28-cloud-native-006-client-todo.md","title":"React Vite Client UI: quick Todo app proof of concept","description":"Create Client UI with Vite React and ViTest","date":"2023-12-28T00:00:00.000Z","formattedDate":"December 28, 2023","tags":[{"label":"Cloud-native","permalink":"/tags/cloud-native"},{"label":"Client UI","permalink":"/tags/client-ui"},{"label":"React","permalink":"/tags/react"},{"label":"Vite and ViTest","permalink":"/tags/vite-and-vi-test"}],"readingTime":10.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023-12-28-cloud-native-006-client-todo.md","canonical_url":"https://dfberry.github.io/blog/2023-12-28-cloud-native-006-client-todo.md","custom_edit_url":null,"sidebar_label":"2023.12-28 Cloud native client","title":"React Vite Client UI: quick Todo app proof of concept","description":"Create Client UI with Vite React and ViTest","published":false,"tags":["Cloud-native","Client UI","React","Vite and ViTest"],"updated":"2023-12-28 00:00 PST"},"prevItem":{"title":"Add a database service to the API project","permalink":"/2024-01-21-cloud-native-008-api-database.md"},"nextItem":{"title":"Deploy to Azure from GitHub with Azure Developer CLI","permalink":"/2023-12-10-deploy-from-github.md"}},"content":"This [sixth iteration][006Code] of the cloud-native project, [https://github.com/dfberry/cloud-native-todo](https://github.com/dfberry/cloud-native-todo), added the client UI to the monorepo. \\n\\n[YouTube demo](https://youtu.be/HTdVSKhnXek)\\n\\n1. Use Vite React to create basic project structure. \\n2. Add React page and components for Todo: form, list, item.\\n3. Add Tests for components. \\n4. Add API integration.\\n\\n> **Reminder:** A quick reminder that this project is using an in-memory DB at this point in the API. Each step of the way is meant to bootstrap the next step for speed instead of complete build out. This step is focusing on a bare-bones UI that interacts with the API. \\n\\n## Front-end framework choices and ChatGPT\\n\\nThis iteration is a proof of concept (POC) that can grow, as opposed to being thrown away. With that in mind, I picked Vite React as the frontend framework. I\'m comfortable with React and I like the Vite toolchain. \\n\\nIn this day and age of ChatGPT everywhere, does it matter what framework you pick for a POC? This is up to you. Whatever answers or code your AI partner (such as ChatGPT) gives you, you still need to be able to integrate it and debug it. I suggest you pick something that you work with as though ChatGPT weren\'t available. If your team knows a different stack, and that stack has any duration (not built in the last year), go with that stack. \\n\\nI considered [Next.js][Nextjs], plain [React][React], [Vite][Vite] React, and [create-react-app][CreateReactApp] (CRA). The POC needs velocity but not at the risk of the velocity or chaos of the underlying stack:\\n\\n* **Next.js** is a great framework but has its own ideas about the cloud. \\n* **Plain React** means building out my own toolchain -- a waste of time compared to Next, Vite, CRA and other stacks that provide that. \\n* **Create React App** has had some bumps in the road the last few years. Reminds me of the Angular 2,3,4,5 releases which is why I don\'t use Angular anymore. \\n* **Vite** has been dependable in the last few projects so I\'m sticking with that. ChatGPT answers enough of the Vite config and ViTest questions so that\'s a plus. \\n\\n## Creating the basic Vite React app\\n\\nVite has a quick ability to scaffold out the app with the CLI for a variety of front-end frameworks including React, Vue, and Svelte, and Electron. I chose TypeScript and SWC.\\n\\n```bash\\nnpm create vite@latest\\n```\\n\\nThis gives a basic runnable app with ESLint already configured. \\n\\n```json\\n{\\n  \\"name\\": \\"vite-project\\",\\n  \\"private\\": true,\\n  \\"version\\": \\"0.0.0\\",\\n  \\"type\\": \\"module\\",\\n  \\"scripts\\": {\\n    \\"dev\\": \\"vite\\",\\n    \\"build\\": \\"tsc && vite build\\",\\n    \\"lint\\": \\"eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0\\",\\n    \\"preview\\": \\"vite preview\\"\\n  },\\n  \\"dependencies\\": {\\n    \\"react\\": \\"^18.2.0\\",\\n    \\"react-dom\\": \\"^18.2.0\\"\\n  },\\n  \\"devDependencies\\": {\\n    \\"@types/react\\": \\"^18.2.43\\",\\n    \\"@types/react-dom\\": \\"^18.2.17\\",\\n    \\"@typescript-eslint/eslint-plugin\\": \\"^6.14.0\\",\\n    \\"@typescript-eslint/parser\\": \\"^6.14.0\\",\\n    \\"@vitejs/plugin-react-swc\\": \\"^3.5.0\\",\\n    \\"eslint\\": \\"^8.55.0\\",\\n    \\"eslint-plugin-react-hooks\\": \\"^4.6.0\\",\\n    \\"eslint-plugin-react-refresh\\": \\"^0.4.5\\",\\n    \\"typescript\\": \\"^5.2.2\\",\\n    \\"vite\\": \\"^5.0.8\\"\\n  }\\n}\\n```\\n\\nThe `vite.config.ts` is where all the configuration goes.\\n\\n## Add environment variable for API\\n\\nCreate a `.env` file and add an environment variable prefixed with `VITE_` for the API URL such as `http://localhost:3000`. When the client is deployed to the host, this URL will need to be changes and the front-end client build with the correct cloud URL. This URL is used later to build out the full API URL to fetch results:\\n\\n```typescript\\nconst ENV_URL = import.meta.env.VITE_API_URL || \'http://localhost:3000\';\\nif(!ENV_URL) {\\n  console.log(\'VITE_API_URL is not defined\');\\n}\\n\\nexport const API_URL = `${ENV_URL}/todo`;\\n```\\n\\nFor this POC, a simple API service looks like: \\n\\n```typescript\\nimport { NewTodo } from \'./models\';\\n\\nconst ENV_URL = import.meta.env.VITE_API_URL || \'http://localhost:3000\';\\nif(!ENV_URL) {\\n  console.log(\'VITE_API_URL is not defined\');\\n}\\n\\nexport const API_URL = `${ENV_URL}/todo`;\\n\\nexport const addTodo = async (newTodo: NewTodo): Promise<Response> => {\\n    return await fetch(API_URL, {\\n        method: \'POST\',\\n        headers: {\\n          \'Content-Type\': \'application/json\',\\n        },\\n        body: JSON.stringify(newTodo),\\n      });\\n    };\\nexport const deleteTodo = async (id: number): Promise<Response> => {\\n    return await fetch(`${API_URL}/${id}`, {\\n        method: \'DELETE\',\\n      });\\n    };\\n```\\n\\n## Clean up the app\\n\\nThe main boilerplate for the Vite React app has a few things going on but none of which this POC needs at this point.\\n\\n```JSX\\nimport { useState } from \'react\'\\nimport reactLogo from \'./assets/react.svg\'\\nimport viteLogo from \'/vite.svg\'\\nimport \'./App.css\'\\n\\nfunction App() {\\n  const [count, setCount] = useState(0)\\n\\n  return (\\n    <>\\n      <div>\\n        <a href=\\"https://vitejs.dev\\" target=\\"_blank\\">\\n          <img src={viteLogo} className=\\"logo\\" alt=\\"Vite logo\\" />\\n        </a>\\n        <a href=\\"https://react.dev\\" target=\\"_blank\\">\\n          <img src={reactLogo} className=\\"logo react\\" alt=\\"React logo\\" />\\n        </a>\\n      </div>\\n      <h1>Vite + React</h1>\\n      <div className=\\"card\\">\\n        <button onClick={() => setCount((count) => count + 1)}>\\n          count is {count}\\n        </button>\\n        <p>\\n          Edit <code>src/App.tsx</code> and save to test HMR\\n        </p>\\n      </div>\\n      <p className=\\"read-the-docs\\">\\n        Click on the Vite and React logos to learn more\\n      </p>\\n    </>\\n  )\\n}\\n\\nexport default App\\n```\\n\\nReplace the contents with a pared-down component:\\n\\n```JSX\\nimport \'./App.css\'\\n\\nimport Todo from \'./todo\'\\n\\nfunction App() {\\n\\n  return (\\n    <>\\n        <Todo />\\n    </>\\n  )\\n}\\n\\nexport default App\\n```\\n\\n## Add the page, form, list, and item \\n\\n1. To keep the client UI clean and clear, create a new subfolder for everything for the Todo named `todo`.\\n2. Create the main todo page, `index.tsx`, which handled the events, API call, and child rerenders. \\n\\n    ```JSX\\n    import { useState } from \'react\';\\n    import useSWR, { mutate } from \'swr\';\\n    import TodoForm from \'./components/form\';\\n    import List from \'./components/list\';\\n    import { NewTodo, Todo } from \'./models\';\\n    import { API_URL, addTodo, deleteTodo } from \'./service\';\\n    import { fetcher } from \'./api\';\\n\\n    export default function Todo() {\\n        const [requestError, setRequestError] = useState(\'\');\\n        const { data, error, isLoading } = useSWR(API_URL, fetcher)\\n\\n        async function handleSubmit(newTodoItem: NewTodo) {\\n            setRequestError(\'\');\\n\\n            try {\\n                const result = await addTodo(newTodoItem);\\n\\n                if (!result.ok) throw new Error(`result: ${result.status} ${result.statusText}`);\\n                const savedTodo = await result.json();\\n                mutate(API_URL, [...data, savedTodo], false);\\n\\n            } catch (error: unknown) {\\n                setRequestError(String(error));\\n            }\\n        }\\n\\n        async function handleDelete(id: number) {\\n            setRequestError(\'\');\\n            try {\\n                const result = await deleteTodo(id);\\n                if (!result.ok) throw new Error(`result: ${result.status} ${result.statusText}`);\\n                mutate(API_URL, data.filter((todo: Todo) => todo.id !== id), false);\\n            } catch (error: unknown) {\\n                setRequestError(String(error));\\n            }\\n        }\\n\\n        if (error && requestError) return <div>failed to load {error ? JSON.stringify(error) : requestError}</div>\\n        if (isLoading) return <div>loading...{JSON.stringify(isLoading)}</div>\\n\\n        return (\\n            <div>\\n                <TodoForm onSubmit={handleSubmit} requestError={requestError} />\\n                <div>\\n                    <List todos={data} onDelete={handleDelete} />\\n                </div>\\n            </div>\\n        )\\n    }\\n    ```\\n\\n3. Create the listing, `components/list.tsx`, to display the 3 default todos.\\n\\n    ```JSX\\n    import { Todo } from \'../models\';\\n    import Item from \'./item\';\\n\\n    export type { Todo };\\n\\n    interface Props {\\n    todos: Todo[];\\n    onDelete: (id: number) => void;\\n    }\\n\\n    export default function List({ todos, onDelete }: Props) {\\n    return (\\n\\n        todos.length > 0 && (\\n        <table style={{ width: \'100%\', marginTop: \'20px\'}} data-testid=\\"list\\">\\n            <thead>\\n            <tr>\\n                <th >ID</th>\\n                <th >Title</th>\\n                <th >Delete</th>\\n            </tr>\\n            </thead>\\n            <tbody>\\n            {todos.map((todo) => (\\n                <Item\\n                key={todo.id}\\n                todo={todo}\\n                onDelete={onDelete}\\n                />\\n            ))}\\n            </tbody>\\n        </table>\\n        )\\n    )\\n    }\\n    ```\\n\\n4. Add the Item, `components/item.tsx`, to display each item. \\n\\n    ```jsx\\n    import { Todo } from \'../models\';\\n\\n    export type { Todo };\\n\\n    export interface ItemProps {\\n    todo: Todo;\\n    onDelete: (id: number) => void;\\n    }\\n\\n    export default function Item({ todo, onDelete }: ItemProps) {\\n\\n    return (\\n        <tr data-testid=\\"item-row\\">\\n        <td data-testid=\\"item-id\\">{todo.id}</td>\\n        <td data-testid=\\"item-title\\">{todo.title}</td>\\n        <td data-testid=\\"item-delete\\">\\n            <button onClick={() => onDelete(todo.id)} >X</button>\\n        </td>\\n        </tr>\\n    );\\n    }\\n    ```\\n\\n    Notice the attributes for testing, named `data_testid` are included already. \\n\\n5. Add the Form, `components/form.tsx`, to capture a new todo item.\\n\\n    ```jsx\\n    import { FormEvent, KeyboardEvent, ChangeEvent, useRef, useState } from \'react\';\\n    import { NewTodo } from \'../models\';\\n\\n    export type { NewTodo };\\n\\n    interface Props {\\n        onSubmit: (newTodoItem: NewTodo) => void;\\n        requestError?: string;\\n    }\\n    export default function TodoForm({ onSubmit, requestError }: Props) {\\n        const formRef = useRef<HTMLFormElement>(null);\\n        const [newTodo, setNewTodo] = useState<NewTodo>({ title: \'\' });\\n\\n        const handleSubmit = (event: FormEvent<HTMLFormElement>) => {\\n            event.preventDefault();\\n            const formData = new FormData(event.currentTarget);\\n            const title = formData.get(\'title\')?.toString() || null;\\n\\n            if (title !== null) {\\n\\n                onSubmit({\\n                    title\\n                });\\n                if (formRef.current) {\\n                    formRef.current.reset();\\n                }\\n                // Reset the newTodo state\\n                setNewTodo({ title: \'\' });\\n            }\\n        }\\n\\n        const handleKeyDown = (event: KeyboardEvent<HTMLInputElement>) => {\\n            if (event.key === \'Enter\') {\\n                if (formRef.current) {\\n                    formRef.current.dispatchEvent(new Event(\'submit\', { cancelable: true }));\\n                }\\n            }\\n        };\\n        const handleInputChange = (event: ChangeEvent<HTMLInputElement>) => {\\n            setNewTodo({\\n                title: event.target.value,\\n            });\\n        };\\n        return (\\n            <div >\\n                <div>\\n                    <h1 >What do you have to do?</h1>\\n                </div>\\n                <form ref={formRef} onSubmit={handleSubmit} data-testid=\\"todo-form\\">\\n                    <div >\\n                        <input\\n                            id=\\"todoTitle\\"\\n                            name=\\"title\\"\\n                            type=\\"text\\"\\n                            value={newTodo.title}\\n                            placeholder=\\"Title\\"\\n                            onChange={handleInputChange}\\n                            onKeyDown={handleKeyDown}\\n                            data-testid=\\"todo-form-input-title\\"\\n                        />\\n                    </div>\\n                    {requestError && (\\n                        <div data-testid=\\"todo-error\\">\\n                            {requestError}\\n                        </div>\\n                    )}\\n                    <button type=\\"submit\\" disabled={!newTodo.title} data-testid=\\"todo-button\\">Add Todo</button>\\n                </form>\\n            </div>\\n        );\\n    } \\n    ```\\n\\n6. Add any dependency code such as the [API service][006-api-service] and its API [fetcher][006-api-fetcher] for SWR, and the TypeScript [models][006-models] for a new todo and an existing todo. \\n\\n7. Start the API and the client UI to use the form.\\n\\n    ![Browser todo app](media/2023-12-28-cloud-native-006-client-todo/browser-todo-app-ui.png)\\n\\n    The form accepts a title to add a new todo, or deletes a todo using the X on each item\'s room. \\n\\n> **Note:** This UI isn\'t styled and the little style that is there is mostly defaults. If you aren\'t comfortable with CSS or style libraries, use ChatGPT and GitHub CoPilot for this.\\n\\n\\n## Add ViTest UI tests\\n\\nNow that the bare bones proof of concept is working, add the UI tests to validate it. This is important so that any future changes to the app don\'t break existing functionality. \\n\\nThe tests cover the following simple cases:\\n\\n* renders form without error\\n* renders form with error\\n* renders button disabled\\n* renders button enabled\\n* accepts input text\\n* submit form by button\\n* submit form by keypress enter\\n* item component deletes item\\n* renders List with todos\\n* does not render List when todos is empty\\n\\n1. Add [ViTest][ViTest] following the instructions for that site and a few other packages for [testing UI with ViTest][ViTestUI]. Refer to the [package.json][006-ui-package-json] for the complete list.\\n\\n    ```bash\\n    npm install -D vitest @vitest/ui\\n    ```\\n\\n2. Create the `vitest.config.ts` file for configurations:\\n\\n    ```TypeScript\\n    import path from \'node:path\';\\n    import { defineConfig, defaultExclude } from \'vitest/config\';\\n    import configuration from \'./vite.config\';\\n\\n    const config = {\\n        ...configuration,\\n        test: {\\n            reporters: [\'json\', \'default\'],\\n            outputFile: { json: \\"./test-output/test-results.json\\" },\\n            globals: true,\\n            setupFiles: path.resolve(__dirname, \'test/setup.ts\'),\\n            exclude: [...defaultExclude],\\n            environmentMatchGlobs: [\\n                [\'**/*.test.tsx\', \'jsdom\'],\\n                [\'**/*.component.test.ts\', \'jsdom\'],\\n            ]\\n        },\\n    };\\n\\n    export default defineConfig(config);     \\n    ```\\n\\n    The `outputFile` keeps the output files out of the way. The `setupFiles` also keep the test setup files tucked away. \\n\\n3. The hardest part about getting these tests to work was the TypeScript types for the testing library user events such as `await user.type(input, title)`. The test setup and utility files helped with that. If you run into this, make sure to restart your TS Server in Visual Studio Code as well. \\n\\n    ```TypeScript\\n    // test/setup.ts\\n    import \'@testing-library/jest-dom/vitest\';\\n\\n\\n    // test/utilities.ts\\n    import type { ReactElement } from \'react\';\\n    import { render as renderComponent } from \'@testing-library/react\';\\n    import userEvent from \'@testing-library/user-event\';\\n\\n    type RenderOptions = Parameters<typeof renderComponent>[1];\\n\\n    export * from \'@testing-library/react\';\\n\\n    export const render = (ui: ReactElement, options?: RenderOptions) => {\\n        return {\\n            ...renderComponent(ui, options),\\n            user: userEvent.setup(),\\n        };\\n    };\\n    ```\\n\\n4. Then the User event test, such as the following, builds and runs.\\n\\n    ```TypeScript\\n    test(\'submit form by keypress enter\', async () => {\\n\\n        // new title\\n        const title = \'Test Todo\';\\n\\n        // mock add function\\n        const mockAdd = vi.fn();\\n\\n        // render the component\\n        const { user, getByTestId } = render(<TodoForm onSubmit={mockAdd}/>);\\n\\n        // Fill in the input\\n        const input = getByTestId(\'todo-form-input-title\');\\n        await user.type(input, title);\\n\\n        // submit form by keypress\\n        await user.type(input, \'{enter}\');\\n\\n        // todo submitted to parent via onSubmit\\n        expect(mockAdd).toHaveBeenCalledTimes(1);\\n        expect(mockAdd).toHaveBeenCalledWith({ title });\\n    })\\n    ```\\n\\n5. Run the test with `npm run test` and see the results:\\n\\n    ![Visual Studio Code terminal running tests](media/2023-12-28-cloud-native-006-client-todo/visual-studio-code-terminal-vitest-result.png)\\n\\n## Where was CoPilot in this iteration?\\n\\n**Where did CoPilot succeed?** \\n\\nCoPilot came in handy in some of the places that I\'m happy to let to handle: \\n\\n* Quick CSS tweaks - it\'s much faster to play with CSS when CoPilot is generating styles over and over.\\n* Config files - I was surprised by how much CoPilot helped with Vite and ViTest. \\n* Components - it wrote most of the component code, I asked for refactors and it provided those as well.\\n* Tests - it wrote most of the UI tests for me in seconds.\\n\\n**Where did CoPilot fail?** \\n\\nThe tricky parts of integration, especially across tools, dependencies, and versions are still tricky. I spent the most time on the TypeScript issue with the testing library for user events. The fix came from a StackOverflow post which I had to look for. Considering all the layers involved and the time already saved in other places I used CoPilot and ChatGPT, that seems like net positive time savings for a proof of concept. \\n\\n## Where to next? \\n\\nNow that the UI code is written and works locally, the project needs a container for the UI, and it needs to provision the UI resources for that container in the cloud. The client container needs to talk to the API container correctly. Fun stuff!\\n\\n\\n[006-ui-package-json]:https://github.com/dfberry/cloud-native-todo/blob/006-client-todo/client-todo/package.json\\n[006-api-service]:https://github.com/dfberry/cloud-native-todo/blob/006-client-todo/client-todo/src/todo/service.ts\\n[006-api-fetcher]:https://github.com/dfberry/cloud-native-todo/blob/006-client-todo/client-todo/src/todo/api.ts\\n[006-models]:https://github.com/dfberry/cloud-native-todo/blob/006-client-todo/client-todo/src/todo/models.ts\\n[006Code]:https://github.com/dfberry/cloud-native-todo/tree/006-client-todo\\n[Nextjs]:https://nextjs.org/\\n[Vite]:https://vitejs.dev/\\n[ViTest]:https://vitest.dev/guide/\\n[ViTestUI]:https://vitest.dev/guide/ui.html\\n[CreateReactApp]:https://create-react-app.dev/\\n[React]:https://react.dev/"},{"id":"/2023-12-10-deploy-from-github.md","metadata":{"permalink":"/2023-12-10-deploy-from-github.md","source":"@site/blog/2023-12-10-cloud-native-005-deploy-from-github-to-azure.md","title":"Deploy to Azure from GitHub with Azure Developer CLI","description":"Explore the power of Azure Developer CLI in this fifth iteration of our cloud-native project. Learn how to deploy directly from GitHub to Azure, set up a sustainable deployment process, and automate your workflow with `azure-dev.yml`. This post guides you through each step, from configuring your pipeline to deploying manually from GitHub. Dive in to discover how Azure Developer CLI can streamline your cloud deployments.","date":"2023-12-10T00:00:00.000Z","formattedDate":"December 10, 2023","tags":[{"label":"Cloud-native","permalink":"/tags/cloud-native"},{"label":"Deploy","permalink":"/tags/deploy"},{"label":"Azure Developer CLI","permalink":"/tags/azure-developer-cli"},{"label":"Azure","permalink":"/tags/azure"}],"readingTime":5.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023-12-10-deploy-from-github.md","canonical_url":"https://dfberry.github.io/blog/2023-12-10-cloud-native-005-deploy-from-github-to-azure.md","custom_edit_url":null,"sidebar_label":"2023.12-10 Deploy from GitHub","title":"Deploy to Azure from GitHub with Azure Developer CLI","description":"Explore the power of Azure Developer CLI in this fifth iteration of our cloud-native project. Learn how to deploy directly from GitHub to Azure, set up a sustainable deployment process, and automate your workflow with `azure-dev.yml`. This post guides you through each step, from configuring your pipeline to deploying manually from GitHub. Dive in to discover how Azure Developer CLI can streamline your cloud deployments.","published":false,"tags":["Cloud-native","Deploy","Azure Developer CLI","Azure"],"updated":"2023-12-10 00:00 PST"},"prevItem":{"title":"React Vite Client UI: quick Todo app proof of concept","permalink":"/2023-12-28-cloud-native-006-client-todo.md"},"nextItem":{"title":"Supercharging DevOps: Streamlining Cloud Infrastructure with Azure Developer CLI","permalink":"/2023-11-21-cloud-native-devops.md"}},"content":"This [fifth iteration](https://github.com/dfberry/cloud-native-todo/tree/005-deploy-from-github) of the cloud-native project, [https://github.com/dfberry/cloud-native-todo](https://github.com/dfberry/cloud-native-todo), added the changes to deploy from the GitHub repository:\\n\\n[YouTube demo](https://youtu.be/CSZ6dMEkO4Q)\\n\\n1. Add `azure-dev.yml` GitHub action to deploy from source code\\n2. Run `azd pipeline config`\\n    * push action to repo\\n    * create Azure service principal with appropriate cloud permissions\\n    * create GitHub variables to connect to Azure service principal\\n\\n## Setup\\n\\nIn the [fourth iteration](https://dev.to/dfberry/supercharging-devops-streamlining-cloud-infrastructure-with-azure-developer-cli-2o98), the project added the infrastructure as code (IaC), created with Azure Developer CLI with `azd init`. This created the `./azure.yml` file and the `./infra` folder. Using the infrastructure, the project was deployed with `azd up` from the local development environment (my local computer). That isn\'t sustainable or desirable. Let\'s change that so deployment happens from the source code repository.\\n\\n## Add `azure-dev.yml` GitHub action to deploy from source repository\\n\\nThe easiest way to find the correct `azure-dev.yml` is to use the official documentation to find the [template](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/configure-devops-pipeline?tabs=GitHub) closest to your deployed resources and sample. \\n\\n![Browser screenshot of the Azure Developer CLI template table by language and host](media/2023-12-10-cloud-native-005-deploy-from-source-control/browser-azure-developer-cli-template-by-language-and-host.png)\\n\\n1. Copy the contents of the template\'s `azure-dev.yml` file from the sample repository into your own source control in the `./github/workflows/azure-dev.yml` file. \\n\\n    ![Browser screenshot of template source code azure-dev.yml](media/2023-12-10-cloud-native-005-deploy-from-source-control/azure-dev-yml.png)\\n\\n2. Add the **name** to the top of the file if one isn\'t there, such as `name: AZD Deploy`. This helps distinguish between other actions you have the in repository. \\n\\n    ```\\n    name: AZD Deploy\\n\\n    on:\\n      workflow_dispatch:\\n      push:\\n        # Run when commits are pushed to mainline branch (main or master)\\n        # Set this to the mainline branch you are using\\n        branches:\\n          - main\\n          - master\\n    ```\\n\\n3. Make sure the `azure-dev.yml` also has the `workflow_dispatch` as one of the `on` settings. This allows you to deploy manually from GitHub. \\n\\n## Run `azd pipeline config` to create deployment from source repository\\n\\n1. Switch to a branch you intend to be used for deployment such as `main` or `dev`. The current branch name is used to create the federated credentials. \\n2. Run `azd pipeline config`\\n3. If asked, log into your source control.\\n4. When the process is complete, copy the **service principal** name and id. Mine looked something like: \\n\\n    ```\\n    az-dev-12-04-2023-18-11-29 (abc2c40c-b547-4dca-b591-1a4590963066)\\n    ```\\n\\n    When you need to add new configurations, you\'ll need to know either the name or ID to find it in the Microsoft Entra ID in the Azure portal.\\n\\n## Service principal for secure identity\\n\\nThe process created your service principal which is the identity used to deploy securely from GitHub to Azure. If you search for service principal in the Azure portal, it takes you Enterprise app. Don\'t go there. An Enterprise app is meant for other people, like customers, to log in. That\'s a different kind of thing. When you want to find your deployment service principal, search for **Microsoft Entra ID**. \\n    \\n1. Go ahead ... find your service principal in the [Azure portal](https://portal.azure.com/) by searching for **Microsoft Entra ID**. The service principals are listed under the **Manage -> App registrations -> All applications**. \\n2. Select your service principal. This takes you to the **Default Directory | App registrations**.\\n3. On the **Manage -> Certificates & secrets**, view the federated credentials. \\n\\n    ![Browser screenshot of federated credentials](media/2023-12-10-cloud-native-005-deploy-from-source-control/azure-portal-federated-credentials.png)\\n\\n4. On the **Manage -> Roles and Administrators**, view the **Cloud Application Administrator**. \\n\\nWhen you want to remove this service principal, you can come back to the portal, or use [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/)\'s `az ad sp delete --id <service-principal-id>`\\n\\n## GitHub action variables to use service principal\\n\\nThe process added the service principal information to your GitHub repository as action variables. \\n\\n1. Open your GitHub repository in a browser and go to **Settings**.\\n2. Select **Security -> Secrets and variable -> Actions**.\\n3. Select variables to see the service principal variables. \\n\\n    ![Browser screenshot of GitHub repository showing settings page with secure action variables table which lists the values necessary to deploy to Azure securely.]\\n\\n4. Take a look at the actions run as part of the push from the process. The **Build/Test** action ran successfully when AZD pushed the new pipeline file in commit [24f78f4](https://github.com/dfberry/cloud-native-todo/commit/24f78f4336e7bed72801a620176c04f0330b198e). Look for the actions that run based on that commit. \\n\\n    ![Browser screenshot of GitHub actions run with the commit](media/2023-12-10-cloud-native-005-deploy-from-source-control/github-action-initial-deploy-action.png)\\n\\n    Verify that the action ran successfully. Since this was the only change, the application should still have the `1.0.1` version number in the response from a root request. \\n    \\nWhen you want to remove these, you can come back to your repo\'s settings. \\n\\n## Test a deployment from source repository to Azure with Azure Developer CLI\\n\\nTo test the deployment, make a change and push to the repository. This can be in a branch you merge back into the default branch, or you can stay on the default branch to make the change and push. The important thing is that a push is made to the default branch to run the GitHub action.\\n\\nIn this project, a simple change to the API version in the `./api-todo/package.json`\'s **version** property is enough of a change. And this change is reflected in the home route and the returned headers from an API call. \\n\\n1. Change the version from `1.0.1` to `1.0.2`.\\n2. Push the change to main. \\n\\n## Verify deployment from source repository to Azure with Azure Developer CLI\\n\\n1. Open the repository\'s actions panel to see the action to deploy complete. \\n\\n    ![Browser screenshot of actions run from version change and push](media/2023-12-10-cloud-native-005-deploy-from-source-control/github-action-update-version-actions.png)\\n\\n2. Select the **AZD Deploy** for that commit to understand it is the same deployment as the local deployment. Continue to drill into the action until you see the individual steps.\\n\\n    ![Browser screenshot of action steps for deploying from GitHub to Azure from Azure Developer CLI](media/2023-12-10-cloud-native-005-deploy-from-source-control/github-action-azd-deploy-steps-successful.png)\\n\\n3. Select the **Deploy Application** step and scroll to the bottom of that step. It shows the same deployed endpoint for the api-todo as the deployment from my local computer. \\n\\n    ![Browser screenshot of Deploy Application step in GitHub action results](media/2023-12-10-cloud-native-005-deploy-from-source-control/github-action-deploy-application-step.png)\\n\\n4. Open the endpoint in a browser to see the updated version. \\n\\n    ![Browser screenshot of updated application api-todo with new version number 1.0.2](media/2023-12-10-cloud-native-005-deploy-from-source-control/browser-api-todo-app-version-update.png)\\n\\n## Deployment from source code works\\n\\nThis application can now deploy the API app from source code with Azure Developer CLI. \\n\\n## Tips \\n\\nAfter some trial and error, here are the tips I would suggest for this process:\\n\\n* Add a meaningful name to the `azure-dev.yml`. You will have several actions eventually, make sure the name of the deployment action is short and distinct. \\n* Run `azd pipeline config` with the `--principal-name` switch in order to have a meaningful name. \\n\\n## Summary \\n\\nThis was an easy process for such an easy project. I\'m interested to see how the infrastructure as code experience changes and the project changes."},{"id":"/2023-11-21-cloud-native-devops.md","metadata":{"permalink":"/2023-11-21-cloud-native-devops.md","source":"@site/blog/2023-11-21-cloud-native-004-devops.md","title":"Supercharging DevOps: Streamlining Cloud Infrastructure with Azure Developer CLI","description":"Learn how to add infrastructure as code to you project then create resources and deploy code to Azure. ","date":"2023-11-21T00:00:00.000Z","formattedDate":"November 21, 2023","tags":[{"label":"Cloud-native","permalink":"/tags/cloud-native"},{"label":"DevOps","permalink":"/tags/dev-ops"},{"label":"Azure Developer CLI","permalink":"/tags/azure-developer-cli"},{"label":"Playwright","permalink":"/tags/playwright"}],"readingTime":9.215,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023-11-21-cloud-native-devops.md","canonical_url":"https://dfberry.github.io/blog/2023-11-21-cloud-native-devops.md","custom_edit_url":null,"sidebar_label":"2023.11.21 Cloud native DevOps","title":"Supercharging DevOps: Streamlining Cloud Infrastructure with Azure Developer CLI","description":"Learn how to add infrastructure as code to you project then create resources and deploy code to Azure. ","published":false,"tags":["Cloud-native","DevOps","Azure Developer CLI","Playwright"],"updated":"2023-11-21 00:00 PST"},"prevItem":{"title":"Deploy to Azure from GitHub with Azure Developer CLI","permalink":"/2023-12-10-deploy-from-github.md"},"nextItem":{"title":"Embarking on a Cloud-native Journey with a Todo API","permalink":"/2023-11-11-cloud-native-api.md"}},"content":"This [fourth iteration](https://github.com/dfberry/cloud-native-todo/tree/004-devops-azure) of my cloud-native project, [https://github.com/dfberry/cloud-native-todo](https://github.com/dfberry/cloud-native-todo), added the steps of creating the cloud resources (provisioning) and pushing code to those resources (deployment). \\n\\n![Diagram showing local and cloud areas with actions of provision and deployment between them.](./media/2023-11-21-cloud-native-004-devops/diagram-devops-actions.png)\\n\\nFor this cloud-native project, I knew there would be a Docker image of the project in a registry but I wasn\'t sure of the fastest steps to create the image from the repository, push it to the registry or how it was pulled into the hosting environment. The authentication part to push to a registry and from which tool is usually what takes a minute or two. Anything that improved that auth flow would be welcome. \\n\\nSticking with tools I know to go as fast as possible, I used [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/) for the infrastructure. \\n\\n\\n## Install Azure Developer CLI as a dev container feature in Visual Studio Code\\n\\nInstallation of Azure Developer CLI into dev containers is easy with a feature. [Find the feature](https://containers.dev/features) and add it to the `./.devcontainer/devcontainer.json`. \\n\\n```json\\n// Features to add to the dev container. More info: https://containers.dev/features.\\n\\"features\\": {\\n    \\"ghcr.io/azure/azure-dev/azd:latest\\": {}\\n},\\n```\\n\\nUse the Visual Studio Code command palette to select **Dev Containers: Rebuild and reopen in container**. Check the version of the Azure Developer CLI installed with the following command: \\n\\n```bash\\nazd version\\n```\\n\\nThe response: \\n\\n```bash\\nazd version 1.5.0 (commit 012ae734904e0c376ce5074605a6d0d3f05789ee)\\n```\\n\\n## Create the infrastructure code with Azure Developer CLI\\n\\nI\'ve done most of this work before in other projects. I didn\'t really expect to learn anything new. However, [GitHub Universe 2023](https://www.youtube.com/watch?v=NrQkdDVupQE&list=PL0lo9MOBetEGF_pCQVCc_3z36ihKSolLC) and [Microsoft Ignite 2023](https://www.youtube.com/watch?v=FZhbJZEgKQ4&list=PLFPUGjQjckXE2cf8RBSjFYUUq8HkM_3zW) both took place between [iteration 003](https://dfberry.github.io/2023-11-11-cloud-native-api.md) and my start on this iteration, 004. While I still used [Copilot Chat](https://docs.github.com/en/copilot) as my pair programming buddy, I also leaned into any new feature I heard of from these two industry conferences. The Azure Developer CLI\'s `azd init` feature had an update (version 1.50) and I wanted to see what it would do. It asked Copilot Chat a couple of questions then it created the required files and folders. It took hours of Bicep development and compressed it into 30 seconds. Amazing!!!\\n\\n![Screenshot of Visual Studio Code using azd init to create infrastructure of project.](./media/2023-11-21-cloud-native-004-devops/visual-studio-code-teminal-azd-init.png)\\n\\nDid it correctly configure the infrastructure for this project? Yes. When I add a second app to this project, further down the road, I\'ll rerun `azd init` in a new branch. \\n\\nThe `azd init` process created a `./next-steps.md` which was a huge help in validation. \\n\\n![Screenshot of Visual Studio Code displaying next steps file](./media/2023-11-21-cloud-native-004-devops/visual-studio-code-next-steps.png)\\n\\n## Get cloud resource environment variables from Azure Developer CLI\\n\\nThe [next steps](https://github.com/dfberry/cloud-native-todo/blob/main/next-steps.md) covered environment variables because your project may need access to cloud resource secrets, connection strings, resource names, database names, and other settings created during provisioning to complete deployment tests. Azure Developer CLI gives you access this list of environment variables with `azd env get-values` to create your own `.env` file for your project. \\n\\nI created a Bash script to get those values so I could test the endpoint.\\n\\n```bash\\n#!/bin/bash\\n# Usage: <script> <path-for-env-file>\\n# Example: ./scripts/postdeploy.sh \\"./api-todo-test\\" \\necho \\"postdeploy.sh\\"\\n\\nset -x\\n\\necho \\"Getting param 1\\"\\nENV_PATH=\\"$1/.env\\" || \\".env\\"\\necho \\"ENV_PATH: $ENV_PATH\\"\\n\\necho \\"Remove old .env file\\"\\nrm -f $ENV_PATH\\n\\necho \\"Getting values from azd\\"\\nazd env get-values > $ENV_PATH\\n\\n# Check if the .env exists\\nif [ ! -f \\"$ENV_PATH\\" ]; then\\n  echo \\"*** .env file not found at $1\\"\\n  exit 1\\nfi\\n\\n# Run the npm test command\\necho \\"Run test at $1\\"\\ncd \\"$1\\" && npm test\\n\\necho \\"Test completed\\"\\nexit 0\\n```\\n\\nThis script is called in the `./azure.yaml` file in the post deployment hook:\\n\\n```yaml\\npostdeploy: \\n    shell: sh\\n    run: |\\n        echo \\"***** Root postdeploy\\"\\n        ./scripts/postdeploy.sh \\"./api-todo-test\\"  \\n```\\n\\n## Develop containers for cloud-native apps\\n\\nWhen I tried to use Azure Developer CLI to provision the project with `azd up`, the provision failed because the CLI couldn\'t find the tools in the environment to build and push the image to the Azure Container Registy. \\n\\n![Screenshot of Visual Studio Code terminal displaying result of azd up as docker tools are missing](./media/2023-11-21-cloud-native-004-devops/visual-studio-code-terminal-docker-missing.png)\\n\\nWhile Docker isn\'t specifically required to run Azure Developer CLI, it\'s logical to assume if I intend to create images, I need the tools to do that. Copilot advised me to create a new `Dockerfile` for the dev container. This would have added another level of complexity and maintenance. Instead, I chose to use a dev container feature for [docker-in-docker](https://github.com/devcontainers/features/tree/main/src/docker-in-docker) which leaves that complexity to the owner of the feature. \\n\\n## Fix for dev container won\'t start\\n\\nI love Docker and I love dev containers but occasionally containers just don\'t start and the error messages are so low-level that they generally aren\'t helpful. The whole point of containers is that they consistently work but I develop on a Mac M1 and containers sometimes don\'t work well with M1. \\n\\nWhen I added the docker-in-docker feature to the Visual Studio dev container and rebuilt the container, the container wouldn\'t start. I changed the configs and looked at the order of features, searched StackOverflow and GitHub and chatted with Copilot. Nothing helped. Using Visual Studio Code to rebuild the dev container without the cache didn\'t fix it either. Which is when I knew it was my environment.\\n\\nThe fix was to stop the dev container, delete all containers, images, and volumes associated with the dev container and start over completely. I didn\'t have any other projects in dev containers so I removed everything. \\n\\n```bash\\n# Delete all containers\\ndocker rm -f $(docker ps -a -q)\\n\\n# Delete all images\\ndocker rmi -f $(docker images -a -q)\\n\\n# Delete all volumes\\ndocker volume rm $(docker volume ls -q)\\n```\\n\\n## Deploy Express.js container image to Azure\\n\\nRestart the dev container and the dev container started. At this point, I tried to provision again with `azd up` (provision & deploy) which succeeded. It\'s impressive how the services just work together without me having to figure out how to pass integration information around. \\n\\n![Screenshot of Visual Studio Code with successful deployment](./media/2023-11-21-cloud-native-004-devops/visual-studio-code-terminal-azd-up.png)\\n\\nThen I tried the endpoint for the API which is shown at the end of the output when `azd up` is done. I didn\'t get my project from the endpoint. The \\"Hello World\\" for containers responded at the endpoint which meant provisioning worked but deployment failed. \\n\\n## Find container image deployment error in Azure portal deployment log\\n\\nThe Azure resource group, the logical unit for all the resources in the infrastructure, has a deployment log. The Container App showed a failed status. The code is still a very simple Express.js app so the issue had to also be simple. I checked the deployment logs in the Azure portal and found the app\'s start script pointed to the wrong file. \\n\\n![Azure portal Container App deployment revision error](./media/2023-11-21-cloud-native-004-devops/azure-portal-failed-deployment.png)\\n\\nFollowing the error to the log shows the issue that the start file is incorrect.\\n\\n![Azure portal Container App logs](./media/2023-11-21-cloud-native-004-devops/azure-portal-log-container-fail.png)\\n\\nA quick fix to the Dockerfile.\\n\\n```yaml\\n# Wrong cmd\\nCMD [ \\"pm2-runtime\\", \\"start\\", \\"server.js\\" ]\\n\\n# Correct cmd\\nCMD [ \\"pm2-runtime\\", \\"start\\", \\"dist/start.js\\" ]\\n```\\n\\nThen `azd up` and the correct endpoint worked. \\n\\n## Add a version header to source code \\n\\nWhile testing the deployment, I wanted to add versioning to the app so I knew changes to the project were displayed at the endpoint. The root request returns the version found in the `./api-todo/package.json`, and the APIs return a `x-api-version` header with the value. \\n\\n```typescript\\n// eslint-disable-next-line @typescript-eslint/ban-ts-comment\\n// @ts-ignore: Ignoring TS6059 as we want to import version from package.json\\nimport { version } from \'../../package.json\';\\n\\nexport function setVersionHeader(_, res, next) {\\n  res.setHeader(\'x-api-version\', version);\\n  next();\\n}\\n\\n```\\n\\nThe **curl** request returns the header when using `--verbose`.\\n\\n![Visual Studio code terminal with curl request showing x-api-version header displaying version 1.0.1](./media/2023-11-21-cloud-native-004-devops/visual-studio-code-curl-request.png)\\n\\n## Add Playwright test to validate API\\n\\nWhen I added playwright to the dev container and rebuilt the dev container, the container started but Playwright and its dependencies took up too much room. I increased the size of my container and limited by testing to Chrome. I also added the installation in the `./.devcontainer/post-create-command.sh` script. By adding the installation here, when the container opens, I can see if it has enough room for a big dependency like Playwright and its browsers. \\n\\n```bash\\n# ./.devcontainer/post-create-command.sh\\n\\n#! /bin/bash\\nsudo apt-get clean\\nsudo apt update\\nnpm i -g npm@latest\\nnpm install\\nchmod -R +x ./scripts\\nnpx playwright install --with-deps \\necho \\"Node version\\" && node -v\\necho \\"NPM version\\" && npm -v\\necho \\"Git version\\" && git -v\\necho \\"Docker version\\" && docker --version \\n```\\n\\nThe Playwright for the API tests the new header and the returned array of todos.\\n\\n```typescript\\nimport { test, expect } from \'@playwright/test\';\\nimport dotenv from \'dotenv\';\\ndotenv.config();\\n\\nconst API_URL = process.env.API_TODO_URL || \'http://localhost:3000\';\\nconsole.log(\'API_URL\', API_URL);\\n\\nimport { version } from \'../../api-todo/package.json\';\\n\\ntest.use({\\n  ignoreHTTPSErrors: true, // in case your certificate isn\'t properly signed\\n  baseURL: API_URL,\\n  extraHTTPHeaders: {\\n    \'Accept\': \'application/vnd.github.v3+json\',\\n    // Add authorization token to all requests.\\n    \'Authorization\': `token ${process.env.API_TOKEN}`,\\n  }\\n});\\ntest(\'should get all todos\', async ({ request }) => {\\n  const response = await request.get(`/todo`);\\n  expect(response.ok()).toBeTruthy();\\n\\n  // Validate the x-api-version header\\n  const headers = response.headers();\\n  expect(headers).toHaveProperty(\'x-api-version\');\\n  expect(headers[\'x-api-version\']).toEqual(version);\\n\\n\\n  // Validate the response body\\n  const todos = await response.json();\\n  expect(Array.isArray(todos)).toBeTruthy();\\n  expect(todos.length).toEqual(3);\\n});\\n```\\n\\nRun the test from the workspace with `npm run test --workspace=api-todo-test` and see the test succeeded. \\n\\n![Screenshot of Visual Studio Code terminal with Playwright test results](./media/2023-11-21-cloud-native-004-devops/visual-studio-code-terminal-playwright-results.png)\\n\\n## Most fun - time savings\\n\\nThe best part about this project is the tooling. I can spend less time and enjoy that time more. \\n\\n![Cartoonish image of a clock](./media/2023-11-21-cloud-native-004-devops/time-savings.jpg)\\n\\nCurrently Copilot shines with technologies that have a lot of Internet coverage including docs and troubleshooting. For this particular iteration, the only place Copilot didn\'t help was the annoying Docker issue when the dev container wouldn\'t start after adding the docker-in-docker dev container feature. \\n\\n## Wish list item #1 - `azd test`\\n\\nWhile Azure Developer CLI provided provisioning and deployment, it didn\'t add testing. This seems like a natural next step for the project. It knows what the stack is because it created the infrastructure to support it. And it knows the endpoints because it displays them at the end of the deployment. Adding API tests seems within the tool\'s ability someday.\\n\\n## Wist list item #2 - docker-in-docker\\n\\nSince the infrastructure required containers and the environment had the `.devcontainer` folder, adding docker-in-docker as a dev container feature is probably something Azure Developer CLI can fix in the future...perhaps a YAML snippet for the dev container feature in the `./next-steps.md`:\\n\\n```json\\n\\"features\\": {\\n    \\"ghcr.io/azure/azure-dev/azd:latest\\": {},\\n    \\"ghcr.io/devcontainers/features/docker-in-docker:1\\":{}\\n},\\n```\\n\\n## Tips\\n\\nThere were a few things I found useful that I use moving forward in my development in the [Tips list](https://github.com/dfberry/cloud-native-todo).\\n\\n## Results for 004 - create resources and deploy code\\n\\nOnce again Copilot saved a lot of time but it took backseat to the amazing work Azure Developer CLI provided with the entire DevOps flow. And notice there wasn\'t any auth flow for the Container registry to deal with when pushing images. That was all wrapped up in the Azure Developer CLI auth. Another time saver."},{"id":"/2023-11-11-cloud-native-api.md","metadata":{"permalink":"/2023-11-11-cloud-native-api.md","source":"@site/blog/2023-11-10-cloud-native-003-api.md","title":"Embarking on a Cloud-native Journey with a Todo API","description":"Unleash the power of Visual Studio Code and GitHub Copilot to create a Todo API.","date":"2023-11-10T00:00:00.000Z","formattedDate":"November 10, 2023","tags":[{"label":"Cloud-native","permalink":"/tags/cloud-native"},{"label":"AI assisted","permalink":"/tags/ai-assisted"},{"label":"todo","permalink":"/tags/todo"},{"label":"api","permalink":"/tags/api"}],"readingTime":3.365,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023-11-11-cloud-native-api.md","canonical_url":"https://dfberry.github.io/blog/2023-11-10-cloud-native-api.md","custom_edit_url":null,"sidebar_label":"2023.11.11 Cloud native API","title":"Embarking on a Cloud-native Journey with a Todo API","description":"Unleash the power of Visual Studio Code and GitHub Copilot to create a Todo API.","published":false,"tags":["Cloud-native","AI assisted","todo","api"],"updated":"2023-11-11 00:00 PST"},"prevItem":{"title":"Supercharging DevOps: Streamlining Cloud Infrastructure with Azure Developer CLI","permalink":"/2023-11-21-cloud-native-devops.md"},"nextItem":{"title":"Cloud-native journey - developer environment setup","permalink":"/2023-11-03-cloud-native-002-developer-environment-setup"}},"content":"Our cloud-native adventure begins with the API layer - the magical bridge between the front-end UI and the back-end services. For our Todo project, we\'re keeping the API simple and efficient. Express.js is our chosen framework, a tried-and-true Node.js project. With the power of Copilot Chat, we\'ll be speeding through the process in no time!\\n\\n[How do you typically approach building a new API for a cloud-native project?](https://bit.ly/3SBV3zx)\\n\\n## Fire Up the Dev Container\\n\\nIn the previous chapter of our journey, [002-developer-environment-setup](https://github.com/dfberry/cloud-native-todo/tree/002-developer-environment-setup), we set up a robust dev environment. Now, it\'s time to bring it to life! Open it in GitHub Codespaces or locally on your computer with Visual Studio Code (Docker installation required). \\n\\nWhether you\'re the lead developer or part of a team, whether you\'re working on a familiar project or exploring new territories, you\'ve got options. Visual Studio Code and the dev container for local work, or Codespaces for a cloud-based approach.\\n\\n\\n## Stay in the dev container\\n\\nIf you are like me, your local computer may not be a workhorse so docker may not be started when you start working on your project. You can start using Copilot chap locally, then realize you need the dev container for something. The Copilot chat stays with the environment, it doesn\'t move (at this time). If you are 20 questions into your conversation with a few side trips here and there, switching environments and not having the chat to reference is frustrating. \\n\\nIf you are using dev containers and Copilot chat, start and stay in the container for the entire conversation.\\n\\n## Building an API with Copilot Chat\\n\\nIn just half an hour, Copilot Chat helped me create a fully functional API, complete with types, linting, tests, and a build-test workflow. Here\'s a sneak peek into the prompts I used:\\n\\n1. Building a todo microservice with Node.js and TypeScript.\\n1. Adding tests for the todo API.\\n1. Refactoring server.ts for both server and test.\\n1. Modifying server.ts for CRUD operations.\\n1. Providing initial sample data.\\n1. Creating an OpenAPI yaml for the API.\\n1. Adding an OpenAPI UI route.\\n1. Setting up ESLint with Prettier.\\n1. Deciding .gitignore contents.\\n1. Moving openapi.yaml to the dist folder using tsc.\\n1. Creating a GitHub action for linting, building, and testing.\\n1. Identifying missing microservice elements.\\n1. Adding type safety.\\n\\n## Refining Types and Refactoring\\n\\nThere was some back-and-forth over types and refactoring. Copilot shone in evaluating incoming API request data. After a few prompts, the validation looked like this:\\n\\n```typescript\\n\\nexport interface Todo {\\n  id: number;\\n  title: string;\\n}\\n\\nexport interface PartialTodo {\\n  id?: unknown;\\n  title?: unknown;\\n}\\n\\nconst todoSchema = Joi.object({\\n  id: Joi.number().greater(0).required(),\\n  title: Joi.string().min(1).max(1000).required(),\\n}).unknown(false);\\n\\nconst todoPartialSchema = Joi.object({\\n  id: Joi.number().greater(0),\\n  title: Joi.string().min(1).max(1000).required(),\\n}).unknown(false);\\n\\nexport interface TodoValidation {\\n  valid: boolean;\\n  error: Error | null | string | ValidationErrorItem[];\\n  todo: Todo | PartialTodo | null;\\n}\\n\\nexport const validateTodo = (\\n  todo: PartialTodo,\\n  isNewTodo: boolean = false\\n): TodoValidation => {\\n  const schema = isNewTodo ? todoPartialSchema : todoSchema;\\n  const { error } = schema.validate(todo);\\n  if (error) {\\n    return {\\n      valid: false,\\n      error: error.details,\\n      todo: null,\\n    };\\n  }\\n  return { valid: true, error: null, todo };\\n};\\n```\\n\\nCopilot also excelled in creating logging handlers for requests and responses, and in adding those handlers to the route.\\n\\n## Copilot\'s Strengths\\nCopilot gave me a flying start. While some answers lacked details, running the app or tests quickly revealed any errors, which were easy to fix.\\n\\n## Copilot\'s Weaknesses\\nDespite the conversation and the wealth of examples, I encountered more issues than expected. For more obscure subjects, I\'d recommend breaking down the steps more atomically.\\n\\n## Why Not Use Existing Code?\\nSure, there are plenty of examples on GitHub. But navigating licenses and attributions can be tricky. I preferred to avoid any potential missteps.\\n\\n## Time Investment\\nFrom start to finish, the project took about 2 hours. There were a few hiccups along the way, but each step was small, making issues manageable. Considering everything, 2 hours is a solid benchmark for a proof-of-concept project."},{"id":"/2023-11-03-cloud-native-002-developer-environment-setup","metadata":{"permalink":"/2023-11-03-cloud-native-002-developer-environment-setup","source":"@site/blog/2023-11-03-cloud-native-002-developer-environment-setup.md","title":"Cloud-native journey - developer environment setup","description":"Set up developer environment with Docker dev containers for Visual Studio Code.","date":"2023-11-03T00:00:00.000Z","formattedDate":"November 3, 2023","tags":[{"label":"Cloud-native","permalink":"/tags/cloud-native"},{"label":"Cloud","permalink":"/tags/cloud"},{"label":"Beginner","permalink":"/tags/beginner"},{"label":"Concept","permalink":"/tags/concept"},{"label":"AI assisted","permalink":"/tags/ai-assisted"},{"label":"todo","permalink":"/tags/todo"},{"label":"devex","permalink":"/tags/devex"},{"label":"setup","permalink":"/tags/setup"}],"readingTime":5.955,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023-11-03-cloud-native-002-developer-environment-setup","canonical_url":"https://dfberry.github.io/blog/2023-11-03-cloud-native-002-developer-environment-setup","custom_edit_url":null,"sidebar_label":"2023.11.03 Dev setup","title":"Cloud-native journey - developer environment setup","description":"Set up developer environment with Docker dev containers for Visual Studio Code.","published":false,"tags":["Cloud-native","Cloud","Beginner","Concept","AI assisted","todo","devex","setup"],"updated":"2023-11-03 00:00 PST"},"prevItem":{"title":"Embarking on a Cloud-native Journey with a Todo API","permalink":"/2023-11-11-cloud-native-api.md"},"nextItem":{"title":"Cloud-native journey - introduction","permalink":"/2023-10-27-cloud-native-introduction"}},"content":"While this project is focused on cloud-native, I set up my environment the same way for every project, with GitHub repositories and [dev containers](https://containers.dev/). This allows me to move from my Windows laptop, to my Mac M1, and on to [GitHub Codespaces](https://github.com/features/codespaces) when demo-ing. \\n\\n[YouTube video](https://youtu.be/oL-K-x0mp3Y)\\n\\nHow do you ensure a consistent developer setup on your team? [Survey](https://bit.ly/3FL6FZ6)\\n\\n## Visual Studio Code + dev containers\\n\\nIf you haven\'t heard of dev containers, let me welcome you to the club. I \u2764\ufe0f  dev containers. \\n\\n\\"A development container (or dev container for short) allows you to use a container as a full-featured development environment.\\"\\n    \\nVisual Studio Code has taken this idea, realized how hard Docker can be for newbies, and sorted it out so it is generally painless for beginners. By the time you want to expand beyond the beginner environment, you are ready to ask the right questions. \\n\\n## Meet my good friend Docker \\n\\nImagine you want to work in environments other than your current computer, and you don\'t want to deal with the time and complexity of installations and configurations. A general configuration will do. That\'s kind of the mentality that eventually (down a long dark road of VirtualBox and Vagrant) led to Docker. \\n\\nThere are many (many!!!) versions of Docker images on [Docker Hub](https://hub.docker.com/) for you. This generally just means knowing an image\'s name. Some images are from manufacturers and some are from community contributions, with improvements and opinions about what the image should have. \\n\\nVisual Studio Code also has some images, meant specifically for development environments. The base image supports a lot of programming languages and tools so don\'t feel like you need to research images just yet. The basic image will do. \\n\\n1. Make sure Docker is running on your computer.\\n2. Make sure you have the [Visual Studio Code\'s Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) added.\\n3. Pick a [Dev containers quickstart](https://code.visualstudio.com/docs/devcontainers/containers#_picking-your-quick-start) in the Visual Studio Docs. \\n4. My favorite is the [Open an existing folder in a container](https://code.visualstudio.com/docs/devcontainers/containers#_quick-start-open-an-existing-folder-in-a-container).\\n5. The steps ask you questions and you answer. Its straightforward and to the point. No cumbersome CLI with many switches to understand. \\n6. It picks an image for you and asks you if you need anything else. So early in the project, you don\'t. You can always come back later and check a box if you need something. \\n\\n## Where\'s the AI?\\n\\nThis is pretty routine for me - I\'ve been in Dockerland for many years so I haven\'t necessarily asked Copilot for help. However, you can always ask Copilot for advice or next steps. Make sure you have any relevant code in the main Visual Studio editor so Copilot can find the best answer. \\n\\n**Prompt**: I need to set up a Todo microservice. What Visual Studio Code dev container would you suggest? \\n\\n**GitHub Copilot**: For a JavaScript project using npm and React, I would suggest using a Node.js based dev container. Visual Studio Code has a predefined Node.js & TypeScript dev container that could be a good starting point.\\n\\nHere\'s a basic setup:\\n\\n1. Install the \\"Remote - Containers\\" extension in Visual Studio Code.\\n2. Open your project folder in VS Code.\\n3. Press F1 to open the command palette and run the \\"Remote-Containers: Add Development Container Configuration Files...\\" command.\\n4. Choose \\"Node.js & TypeScript\\" from the list.\\n\\n**My perspective**: This isn\'t the entire response. Copilot immediately went into dense Docker file syntax. Visual Studio Code does a great job of keeping you out of that. \\n\\n## My dev container setup for Node.js and TypeScript\\n\\nHere is how I set up my dev container: \\n\\n1. I brought the [dfberry/cloud-native-todo](https://github.com/dfberry/cloud-native-todo) down to my local computer. You should fork then clone it if you want to do the same.\\n2. Open the local repository in Visual Studio Code.\\n3. Docker is running and the [Visual Studio Code\'s Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) is installed. \\n4. Reopen the folder in a **New Dev Container**.\\n5. That pops up a list of container configuration templates. \\n6. I selected the one which uses my language of choice: **Node.js & TypeScript** and an appropriate version (18). If you choose a different template, your questions may be different. \\n7. Visual Studio Code reopened with a new terminal prompt, which indicated I was in a dev container environment. \\n8. I checked the Node.js version just to make sure: `node --version` and got back `v18.18.0`. My dev container works and all I did was answer questions. For a newbie to Docker and containerization, this is a huge time saver. You will eventually know Docker, but you don\'t need to know it on day 1. \\n\\n## Where is the new dev container? \\n\\nVisual Studio Code added a file to the repository found in the `.devcontainer` folder, named `devcontainer.json`. Open it up. \\n\\nYou\'ll notice some settings are commented out, you\'ll eventually use them, but not just yet. \\n\\nFor now, the most important parts are the `image` which is equivalent to your operating system and its additional installations, and the `customizations` property which is probably empty. \\n\\nSince I know I want to use Copilot as my pair programmer, I need to add that to the environment. In the Visual Studio extensions, search for `GitHub Copilot Chat` and select `Install in Dev Container`. This adds the appropriate setting to the `customizations` property.\\n\\nOne final setting I use a bit is the `postCreateCommand`. I usually add some npm packages for developer experience here but for now I just want to make sure the correct versions of the tools are in the environment. \\n\\nAt this point, my `devcontainer.json` looks like: \\n\\n```json\\n// For format details, see https://aka.ms/devcontainer.json. For config options, see the\\n// README at: https://github.com/devcontainers/templates/tree/main/src/typescript-node\\n{\\n\\t\\"name\\": \\"Node.js & TypeScript\\",\\n\\t// Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile\\n\\t\\"image\\": \\"mcr.microsoft.com/devcontainers/typescript-node:1-18-bookworm\\",\\n\\n\\t// Features to add to the dev container. More info: https://containers.dev/features.\\n\\t\\"features\\": {\\t\\t\\n\\t},\\n\\n\\t// Use \'forwardPorts\' to make a list of ports inside the container available locally.\\n\\t// \\"forwardPorts\\": [],\\n\\n\\t// Use \'postCreateCommand\' to run commands after the container is created.\\n\\t\\"postCreateCommand\\": \\"node -v && npm -v && git -v\\",\\n    \\n\\t// Configure tool-specific properties.\\n\\t\\"customizations\\": {\\n\\t\\t\\"vscode\\": {\\n\\t\\t\\t\\"extensions\\": [\\n\\t\\t\\t\\t\\"GitHub.copilot-chat\\"\\n\\t\\t\\t]\\n\\t\\t}\\n\\t},\\n\\n\\t// Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.\\n\\t\\"remoteUser\\": \\"root\\"\\n}\\n```\\n\\n## Check the dev container into source control\\n\\nNow that you have a working dev container, check it into source control and ask everyone on the team to try it out. This simple container should work on all hosts (`host` is the term for the machine running the container). More complicated dev containers may not work the first and take some tinkering. \\n\\n## Where\'s the Cloud-Native? \\n\\nYou may be wondering why there isn\'t any cloud-native code yet. Don\'t worry, I\'ll get to it. Setting up a dev environment that works for every member of your team paves the way to a smooth development experience. Take the time to make sure the dev container is configured correctly and works for everyone. \\n\\n## Sample Code Repository\\n\\n* [cloud-native-todo repo, 002-developer-environment-setup branch](https://github.com/dfberry/cloud-native-todo/tree/002-developer-environment-setup)\\n\\n## Questions?\\n\\nThis is how I approach a consistent developer experience across a team. How do you do it? Let me know. [Survey](https://bit.ly/3FL6FZ6)"},{"id":"/2023-10-27-cloud-native-introduction","metadata":{"permalink":"/2023-10-27-cloud-native-introduction","source":"@site/blog/2023-10-27-cloud-native-001.md","title":"Cloud-native journey - introduction","description":"Learn what cloud-native is and build a roadmap to implement an application for cloud-native architectures.","date":"2023-10-27T00:00:00.000Z","formattedDate":"October 27, 2023","tags":[{"label":"Cloud-native","permalink":"/tags/cloud-native"},{"label":"Cloud","permalink":"/tags/cloud"},{"label":"Beginner","permalink":"/tags/beginner"},{"label":"Concept","permalink":"/tags/concept"},{"label":"AI assisted","permalink":"/tags/ai-assisted"},{"label":"todo","permalink":"/tags/todo"}],"readingTime":4.165,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023-10-27-cloud-native-introduction","canonical_url":"https://dfberry.github.io/blog/2023-10-27-cloud-native-introduction","custom_edit_url":null,"sidebar_label":"2023.10.27 Cloud-native journey - introduction","title":"Cloud-native journey - introduction","description":"Learn what cloud-native is and build a roadmap to implement an application for cloud-native architectures.","published":true,"tags":["Cloud-native","Cloud","Beginner","Concept","AI assisted","todo"],"updated":"2023-10-27 00:00 PST"},"prevItem":{"title":"Cloud-native journey - developer environment setup","permalink":"/2023-11-03-cloud-native-002-developer-environment-setup"},"nextItem":{"title":"Azure OpenAI Conversational CLI with TypeScript","permalink":"/2023-06-04-azure-openai-cli-with-typescript"}},"content":"I\'ve been working in the **Azure cloud** for a few years now. The term `cloud-native` gets used a lot. In an effort to bridge the gap between Azure cloud and cloud-native, I\'m investing some time in learning cloud-native approaches to building apps. \\n\\nAlong the way, I\'ll use and _experiment_ with practices and tools to expedite the end result. Join me along the way, and let me know how you would have proceeded. \\n\\nTo start with, my tool set includes:\\n\\n* [Visual Studio Code](https://code.visualstudio.com/).\\n* [GitHub](https://github.com/) & [GitHub Copilot](https://github.com/features/copilot) (AI pair programming and rubber duck). Copilot provides context-aware answers within Visual Studio code. \\n\\nThis blog will capture the highlights. For the complete picture, head over to the GitHub repo: [Cloud native repo](https://bit.ly/40aclp8).\\n\\n## What is cloud-native?\\n\\n**Prompt**: What is cloud-native?\\n\\n**GitHub Copilot**: Cloud-native is an approach to building and running applications that takes full advantage of the cloud computing delivery model. It is characterized by the use of containerized applications, microservices architecture, and dynamic orchestration, all of which are designed to be highly scalable, resilient, and agile. The goal of cloud-native is to enable organizations to build and deploy applications faster, with greater flexibility and reliability, while also reducing costs.\\n  \\n## How do I begin? \\n\\n**Prompt**: Where do I start on a new project for cloud-native?\\n\\n**GitHub Copilot**: When starting a new project for cloud-native, there are several steps you can take to ensure that your application is designed for scalability, resilience, and agility. Here are some general steps you can follow:\\n\\n  1. Choose a cloud platform: Select a cloud platform that meets your needs and provides the services and features you require. Some popular cloud platforms include Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).\\n\\n  2. Define your architecture: Define the architecture of your application, including the components, services, and data stores that you will use. Consider using microservices architecture, which can help you build more scalable and resilient applications.\\n\\n  3. Use containers: Use containers to package your application and its dependencies, making it easier to deploy and manage your application across different environments.\\n\\n  4. Use container orchestration: Use a container orchestration platform like Kubernetes to manage your containers and ensure that your application is highly available and scalable.\\n\\n  5. Use automation: Use automation tools like Ansible, Terraform, or CloudFormation to automate the deployment and management of your application.\\n\\n  6. Use monitoring and logging: Use monitoring and logging tools to track the performance and health of your application, and to identify and troubleshoot issues.\\n\\n  7. Use security best practices: Follow security best practices to ensure that your application is secure and protected from attacks.\\n\\nBy following these steps, you can build a cloud-native application that is designed for scalability, resilience, and agility, and that can provide a better experience for your users.\\n\\n## First cloud-native project? \\n\\n**Prompt**: What is a simple project I can use for my first cloud-native project?\\n\\n**GitHub Copilot**: A simple project that you can use for your first cloud-native project is a web application that allows users to create and manage to-do lists. This project can be built using a microservices architecture, with separate services for user authentication, to-do list management, and data storage.\\n\\n**My perspective** \\n\\nA todo list is perfect, there are plenty of examples: \\n\\n* [Todo backend](https://todobackend.com/)\\n* [TodoMVC](https://todomvc.com/)\\n* [Swagger API spec](https://github.com/nickfun/learning-guardrail/blob/master/server.yaml)\\n* [Postman collection of tests](https://github.com/nickfun/learning-guardrail/blob/master/postman-collection.json)\\n\\n## Architecture\\n\\n**Prompt**: Can you suggest a beginning architecture of a todo app which is cloud-native?\\n\\n**GitHub Copilot**: here\'s a beginning architecture for a cloud-native to-do app:\\n\\n1. Frontend: A web or mobile app that allows users to create and manage to-do lists. The frontend can be built using a framework like React, Angular, or Vue.js.\\n\\n2. API Gateway: An API gateway that serves as the entry point for the application and routes requests to the appropriate microservices. The API gateway can be built using a tool like Kong, Istio, or AWS API Gateway.\\n\\n3. Authentication Service: A microservice that handles user authentication and authorization. This service can be built using a tool like Auth0, Okta, or AWS Cognito.\\n\\n4. To-Do Service: A microservice that handles the creation, retrieval, updating, and deletion of to-do items. This service can be built using a framework like Express.js, Spring Boot, or Flask.\\n\\n5. Database: A database that stores the to-do items and user information. The database can be a SQL or NoSQL database like MySQL, PostgreSQL, MongoDB, or DynamoDB.\\n\\n6. Container Orchestration: A container orchestration platform like Kubernetes that manages the deployment and scaling of the microservices. Kubernetes can be used to deploy the microservices as Docker containers and manage their lifecycle.\\n\\n7. Monitoring and Logging: A monitoring and logging solution like Prometheus, Grafana, or ELK Stack that provides visibility into the performance and health of the application.\\n\\nBy following this architecture, you can build a cloud-native to-do app that is designed for scalability, resilience, and agility, and that can provide a better experience for your users.\\n\\n\\n## More resources\\n\\nRead the full conversation: [Cloud native repo - introduction](https://bit.ly/40aclp8)."},{"id":"/2023-06-04-azure-openai-cli-with-typescript","metadata":{"permalink":"/2023-06-04-azure-openai-cli-with-typescript","source":"@site/blog/2023-06-04-openai-cli-with-typescript.md","title":"Azure OpenAI Conversational CLI with TypeScript","description":"Running boilerplate for a Conversational CLI with Azure OpenAI and TypeScript","date":"2023-06-04T00:00:00.000Z","formattedDate":"June 4, 2023","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"OpenAI","permalink":"/tags/open-ai"},{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"Sample","permalink":"/tags/sample"}],"readingTime":5.84,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023-06-04-azure-openai-cli-with-typescript","canonical_url":"https://dfberry.github.io/blog/2023-06-04-azure-openai-cli-with-typescript","custom_edit_url":null,"sidebar_label":"2023.06.04 Azure OpenAI CLI (TS)","title":"Azure OpenAI Conversational CLI with TypeScript","description":"Running boilerplate for a Conversational CLI with Azure OpenAI and TypeScript","published":false,"tags":["Azure","OpenAI","TypeScript","Sample"],"updated":"2023-06-04 05:00 PST"},"prevItem":{"title":"Cloud-native journey - introduction","permalink":"/2023-10-27-cloud-native-introduction"},"nextItem":{"title":"Azure Cloud Shell Frequently asked questions","permalink":"/2023-02-09-azure-cloud-shell-faq"}},"content":"[Azure OpenAI Service](https://learn.microsoft.com/azure/cognitive-services/openai) provides access to OpenAI\'s powerful language models including the GPT-3, Codex and Embeddings model series. These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation. \\n\\n## When to use Azure OpenAI\\n\\nUse this service when you want to use ChapGPT or OpenAI functionality with _your own data and prompts_ which need to remain **private and secure**.\\n\\n## How to use Azure OpenAI programmatically\\n\\nAs with most other Azure services, you can use the REST APIs or language-based SDKs. I wrote my integration code with the REST APIs then converted to the JavaScript/TypeScript SDK, [@azure/openai](https://www.npmjs.com/package/openai), when it released. \\n\\n**Usage tip**: \\n\\n* Use the **REST APIs** when you want to stay on the bleeding edge or use a languages not supported with the SDKs. \\n* Use the SDK when you need the more common integration scenarios and not at the bleeding edge of implementation. \\n\\n## Conversational loops\\n\\nConversational loops like those presented with ChapGPT, OpenAI, and Azure OpenAI are commonly browser-based chats provided by:\\n\\n* [Microsoft Bot Framework](https://github.com/microsoft/botframework-sdk) - .NET and JavaScript/TypeScript\\n* [Power Virtual Agents](https://learn.microsoft.com/en-us/power-virtual-agents/fundamentals-what-is-power-virtual-agents) - Enterprise - No code required\\n\\n## Build a conversational CLI\\n\\nThis conversational CLI interacts with your prompts with a small code-base. This allows you to understand the Azure OpenAI configurations, playing with the knobs and dials, while using the conversational loop and Azure OpenAI SDK to interact with it.\\n\\nRemember to store and pass along the conversation so Azure OpenAI has the context of the full conversation.\\n\\n## Azure OpenAI conversation manager class with TypeScript\\n\\nThis conversation manager class is a starting point to your first Azure OpenAI app. After you [create your Azure OpenAI resource](https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal), you need to pass in your Azure OpenAI endpoint (URL), key, and deployment name to use this class. \\n\\n```typescript\\nimport {\\n  OpenAIClient,\\n  AzureKeyCredential,\\n  GetChatCompletionsOptions\\n} from \'@azure/openai\';\\nimport { DefaultAzureCredential } from \'@azure/identity\';\\n\\nimport {\\n  DebugOptions,\\n  OpenAiAppConfig,\\n  OpenAiConversation,\\n  OpenAiRequest,\\n  OpenAiRequestConfig,\\n  OpenAiResponse,\\n  OpenAiSuccessResponse\\n} from \'./models\';\\nimport { ChatCompletions } from \'@azure/openai\';\\n\\n// export types a client needs\\nexport {\\n  DebugOptions,\\n  OpenAiAppConfig,\\n  OpenAiRequest,\\n  OpenAiRequestConfig,\\n  OpenAiResponse,\\n  OpenAiSuccessResponse\\n} from \'./models\';\\n\\nexport default class OpenAIConversationClient {\\n  #appConfig: OpenAiAppConfig;\\n  #conversationConfig: OpenAiConversation;\\n  #requestConfig: GetChatCompletionsOptions = {\\n    maxTokens: 800,\\n    temperature: 0.9,\\n    topP: 1,\\n    frequencyPenalty: 0,\\n    presencePenalty: 0\\n  };\\n\\n  #openAiClient: OpenAIClient;\\n\\n  constructor(\\n    endpoint: string = process.env.AZURE_OPENAI_ENDPOINT as string,\\n    apiKey: string = process.env.AZURE_OPENAI_API_KEY as string,\\n    deployment: string = process.env.AZURE_OPENAI_DEPLOYMENT as string\\n  ) {\\n    this.#appConfig = {\\n      endpoint,\\n      apiKey,\\n      deployment\\n    };\\n\\n    this.#conversationConfig = {\\n      messages: []\\n    };\\n\\n    if (apiKey && endpoint) {\\n      this.#openAiClient = new OpenAIClient(\\n        endpoint,\\n        new AzureKeyCredential(apiKey)\\n      );\\n    } else {\\n      this.#openAiClient = new OpenAIClient(\\n        endpoint,\\n        new DefaultAzureCredential()\\n      );\\n    }\\n  }\\n\\n  async OpenAiConversationStep(\\n    userText: string,\\n    appOptions?: OpenAiAppConfig | undefined,\\n    requestOptions?: OpenAiRequestConfig | undefined,\\n    debugOptions?: DebugOptions | undefined\\n  ): Promise<OpenAiResponse> {\\n    try {\\n      // REQUEST\\n      const request: OpenAiRequest = {\\n        conversation: {\\n          messages: [\\n            // add all previous messages so the conversation\\n            // has context\\n            ...this.#conversationConfig.messages,\\n            // add the latest user message\\n            {\\n              role: \'user\',\\n              content: userText\\n            }\\n          ]\\n        },\\n        appConfig: appOptions ? appOptions : this.#appConfig,\\n        requestConfig: requestOptions ? requestOptions : this.#requestConfig\\n      };\\n      if (debugOptions?.debug) {\\n        debugOptions.logger(`LIB OpenAi request: ${JSON.stringify(request)}`);\\n      }\\n\\n      // RESPONSE\\n      const response = await this.OpenAiRequest(request);\\n      if (debugOptions?.debug) {\\n        debugOptions.logger(`LIB OpenAi response: ${JSON.stringify(response)}`);\\n      }\\n      return response;\\n    } catch (error: unknown) {\\n\\n      if (error instanceof Error) {\\n        return {\\n          status: \'499\',\\n          error: {\\n            message: error.message,\\n            stack: error.stack\\n          },\\n          data: undefined\\n        };\\n      } else {\\n        return {\\n          status: \'498\',\\n          error: {\\n            message: JSON.stringify(error)\\n          },\\n          data: undefined\\n        };\\n      }\\n    }\\n  }\\n  async OpenAiRequest(request: OpenAiRequest): Promise<OpenAiResponse> {\\n    if (\\n      !request.appConfig.apiKey ||\\n      !request.appConfig.deployment ||\\n      !request.appConfig.endpoint\\n    ) {\\n      return {\\n        data: undefined,\\n        status: \'400\',\\n        error: {\\n          message: \'OpenAiRequest: Missing API Key or Deployment\'\\n        }\\n      };\\n    }\\n\\n    const chatCompletions: ChatCompletions =\\n      await this.#openAiClient.getChatCompletions(\\n        request.appConfig.deployment,\\n        request.conversation.messages,\\n        request.requestConfig\\n      );\\n\\n    return {\\n      data: chatCompletions,\\n      status: \'200\',\\n      error: undefined\\n    };\\n  }\\n}\\n```\\n\\n[Full sample code for Azure OpenAI library](https://github.com/Azure-Samples/azure-typescript-e2e-apps/blob/main/lib-openai/src/index.ts)\\n\\n\\n## Conversational loop\\n\\nNow that the Azure OpenAI library is built, you need a conversational loop. I used [commander](https://www.npmjs.com/package/commander) with [readline\'s question](https://nodejs.org/api/readline.html#rlquestionquery-options) to build the CLI. \\n\\n```typescript\\nimport { Command } from \'commander\';\\nimport * as dotenv from \'dotenv\';\\nimport { writeFileSync } from \'fs\';\\nimport { checkRequiredEnvParams } from \'./settings\';\\nimport OpenAIConversationClient, {\\n  OpenAiResponse,\\n  DebugOptions\\n} from \'@azure-typescript-e2e-apps/lib-openai\';\\nimport chalk from \'chalk\';\\n\\nimport readline from \'node:readline/promises\';\\n\\n// CLI settings\\nlet debug = false;\\nlet debugFile = \'debug.log\';\\nlet envFile = \'.env\';\\n\\n// CLI client\\nconst program: Command = new Command();\\n\\n// ReadLine client\\nconst readlineClient = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout\\n});\\n\\nfunction printf(text: string) {\\n  printd(text);\\n  process.stdout.write(`${text}\\\\n`);\\n}\\nfunction printd(text: string) {\\n  if (debug) {\\n    writeFileSync(debugFile, `${new Date().toISOString()}:${text}\\\\n`, {\\n      flag: \'a\'\\n    });\\n  }\\n}\\n\\nprogram\\n  .name(\'conversation\')\\n  .description(\\n    `A conversation loop\\n\\n        Examples: \\n        index.js -d \'myfile.txt\' -e \'.env\'        Start convo with text from file with settings from .env file\\n    `\\n  )\\n  .option(\\n    \'-d, --dataFile <filename>\',\\n    \'Read content from a file. If both input and data file are provided, both are sent with initial request. Only input is sent with subsequent requests.\'\\n  )\\n  .option(\\n    \'-e, --envFile <filename>. Default: .env\',\\n    \'Load environment variables from a file. Prefer .env to individual option switches. If both are sent, .env is used only.\'\\n  )\\n  .option(\'-l, --log <filename>. Default: debug.log\', \'Log everything to file\')\\n  .option(\'-x, --exit\', \'Exit conversation loop\')\\n  .helpOption(\'-h, --help\', \'Display help\');\\n\\nprogram.description(\'Start a conversation\').action(async (options) => {\\n  // Prepare: Get debug logger\\n  if (options.log) {\\n    debug = true;\\n    debugFile = options?.log || \'debug.log\';\\n\\n    // reset debug file\\n    writeFileSync(debugFile, ``);\\n  }\\n  printd(`CLI Options: ${JSON.stringify(options)}`);\\n\\n  // Prepare: Get OpenAi settings and create client\\n  if (options.envFile) {\\n    envFile = options.envFile;\\n  }\\n  dotenv.config(options.envFile ? { path: options.envFile } : { path: \'.env\' });\\n  printd(`CLI Env file: ${envFile}`);\\n  printd(`CLI Env vars: ${JSON.stringify(process.env)}`);\\n\\n  // Prepare: Check required environment variables\\n  const errors = checkRequiredEnvParams(process.env);\\n  if (errors.length > 0) {\\n    const failures = `${errors.join(\'\\\\n\')}`;\\n    printf(chalk.red(`CLI Required env vars failed: ${failures}`));\\n  } else {\\n    printd(`CLI Required env vars success`);\\n  }\\n\\n  // Prepare: OpenAi Client\\n  const openAiClient: OpenAIConversationClient = new OpenAIConversationClient(\\n    process.env.AZURE_OPENAI_ENDPOINT as string,\\n    process.env.AZURE_OPENAI_API_KEY as string,\\n    process.env.AZURE_OPENAI_DEPLOYMENT as string\\n  );\\n  printd(`CLI OpenAi client created`);\\n\\n  // Prepare: Start conversation\\n  printf(chalk.green(\'Welcome to the OpenAI conversation!\'));\\n\\n  /* eslint-disable-next-line no-constant-condition */\\n  while (true) {\\n    const yourQuestion: string = await readlineClient.question(\\n      chalk.green(\'What would you like to ask? (`exit` to stop)\\\\n>\')\\n    );\\n    // Print response\\n    printf(`\\\\n${chalk.green.bold(`YOU`)}: ${chalk.gray(yourQuestion)}`);\\n\\n    // Exit\\n    if (yourQuestion.toLowerCase() === \'exit\') {\\n      printf(chalk.green(\'Goodbye!\'));\\n      process.exit();\\n    }\\n\\n    await getAnswer(yourQuestion, openAiClient);\\n  }\\n});\\n\\nasync function getAnswer(\\n  question: string,\\n  openAiClient: OpenAIConversationClient\\n): Promise<void> {\\n  // Request\\n  const appOptions = undefined;\\n  const requestOptions = undefined;\\n  const debugOptions: DebugOptions = {\\n    debug: debug,\\n    logger: printd\\n  };\\n\\n  const { status, data, error }: OpenAiResponse =\\n    await openAiClient.OpenAiConversationStep(\\n      question,\\n      appOptions,\\n      requestOptions,\\n      debugOptions\\n    );\\n\\n  // Response\\n  printd(`CLI OpenAi response status: ${status}`);\\n  printd(`CLI OpenAi response data: ${JSON.stringify(data)}`);\\n  printd(`CLI OpenAi response error: ${error}`);\\n\\n  // Error\\n  if (Number(status) > 299) {\\n    printf(\\n      chalk.red(\\n        `Conversation step request error: ${error?.message || \'unknown\'}`\\n      )\\n    );\\n    process.exit();\\n  }\\n\\n  // Answer\\n  if (data?.choices[0]?.message) {\\n    printf(\\n      `\\\\n\\\\n${chalk.green.bold(`ASSISTANT`)}:\\\\n\\\\n${\\n        data?.choices[0].message.content\\n      }\\\\n\\\\n`\\n    );\\n    return;\\n  }\\n\\n  // No Answer\\n  printf(`\\\\n\\\\n${chalk.green.bold(`ASSISTANT`)}:\\\\n\\\\nNo response provided.\\\\n\\\\n`);\\n  return;\\n}\\n\\nprogram.parse(process.argv);\\n```\\n\\n[Full sample code for Conversational loop](https://github.com/Azure-Samples/azure-typescript-e2e-apps/tree/main/cli-openai)\\n\\n## Learn more\\n\\nLearn more about how to [create this Conversational CLI](https://learn.microsoft.com/azure/developer/javascript/openai-cli)."},{"id":"/2023-02-09-azure-cloud-shell-faq","metadata":{"permalink":"/2023-02-09-azure-cloud-shell-faq","source":"@site/blog/2023-02-09-azure-cloud-shell-faq.md","title":"Azure Cloud Shell Frequently asked questions","description":"Things I wish I knew about Azure CLI and Azure Cloud shell","date":"2023-02-09T00:00:00.000Z","formattedDate":"February 9, 2023","tags":[{"label":"Azure","permalink":"/tags/azure"},{"label":"Azure CLI","permalink":"/tags/azure-cli"},{"label":"Azure Cloud Shell","permalink":"/tags/azure-cloud-shell"},{"label":"Tips","permalink":"/tags/tips"}],"readingTime":0.92,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023-02-09-azure-cloud-shell-faq","canonical_url":"https://dfberry.github.io/blog/2023-02-09-azure-cloud-shell-faq","custom_edit_url":null,"sidebar_label":"2023.02.09 Azure Cloud Shell FAQ","title":"Azure Cloud Shell Frequently asked questions","description":"Things I wish I knew about Azure CLI and Azure Cloud shell","published":true,"tags":["Azure","Azure CLI","Azure Cloud Shell","Tips"],"updated":"2023-02-09 05:00 PST"},"prevItem":{"title":"Azure OpenAI Conversational CLI with TypeScript","permalink":"/2023-06-04-azure-openai-cli-with-typescript"},"nextItem":{"title":"TypeScript type guard for empty JSON object","permalink":"/2023.02.04-github-graphql-commit-history-with-typescript"}},"content":"1. You don\'t need to install [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/what-is-azure-cli) in your local dev environment. \\n\\n    The Cloud Shell (Azure CLI in a browser) is available from the [Azure portal](https://portal.azure.com). \\n\\n    ![Screenshot showing Azure Cloud Shell is available from top navigation bar in Azure portal. ](./media/2023-02-09/azure-cloud-shell-button.png)\\n\\n\\n2. The cloud shell is sticky. Because the Cloud shell uses Azure Storage (File storage), when you end your sessions then return, [your files are still there](https://learn.microsoft.com/en-us/azure/cloud-shell/persisting-shell-storage). \\n\\n    * Want to quickly work with a GitHub repo? No problem, **git** is available. \\n\\n\\n3. Because you use it from the portal, you are already authenticated. No need for [az login](https://learn.microsoft.com/cli/azure/reference-index?view=azure-cli-latest#az-login).\\n4. Many CLI tools are already installed for you.\\n    * Azure CLI\\n    * git, zip, jq\\n    * code (not exactly Visual Studio Code, but a good IDE)\\n    * nano, vim\\n    * Node.js, npm\\n    * Java and Maven\\n    * Python\\n    * .NET Core\\n    * PowerShell\\n    * Go (Golang)\\n    * [Azure Functions CLI](https://learn.microsoft.com/azure/azure-functions/functions-run-local)\\n    * Docker CLI, Kubectl, Helm, Terraform, Ansible\\n    * [Office 365 CLI](https://pnp.github.io/cli-microsoft365/)\\n    * MySQL client\\n    * PostgreSql client\\n    * SQL cli\\n\\n5. Create [bash scripts with Azure CLI commands](https://learn.microsoft.com/azure/cloud-shell/quickstart) to manage your Azure resources."},{"id":"/2023.02.04-github-graphql-commit-history-with-typescript","metadata":{"permalink":"/2023.02.04-github-graphql-commit-history-with-typescript","source":"@site/blog/2023-02-04-blog.md","title":"TypeScript type guard for empty JSON object","description":"Safely type through an object\'s type which includes an empty JSON object.","date":"2023-02-04T00:00:00.000Z","formattedDate":"February 4, 2023","tags":[{"label":"TypeScript","permalink":"/tags/type-script"},{"label":"type guard","permalink":"/tags/type-guard"},{"label":"GitHub","permalink":"/tags/git-hub"},{"label":"GraphQL","permalink":"/tags/graph-ql"}],"readingTime":0.91,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"/2023.02.04-github-graphql-commit-history-with-typescript","canonical_url":"https://dfberry.github.io/blog/2023.02.04-github-graphql-commit-history-with-typescript","custom_edit_url":null,"sidebar_label":"2023.02.04 TS + GitHub GraphQL Commit history","title":"TypeScript type guard for empty JSON object","description":"Safely type through an object\'s type which includes an empty JSON object.","published":true,"tags":["TypeScript","type guard","GitHub","GraphQL"],"updated":"2023-02-04 06:21 PST"},"prevItem":{"title":"Azure Cloud Shell Frequently asked questions","permalink":"/2023-02-09-azure-cloud-shell-faq"}},"content":"A commit history for a repo on GitHub can be optional, if there are no commits yet. The TypeScript SDK created by the [GraphQL CodeGen](https://www.npmjs.com/package/@graphql-codegen/cli) represents this optionality is represented with an empty object, null, or undefined. If a commit is present, its represented as a nested JSON object with more optional parameters.\\n\\n```typescript\\ndeclare var x:\\n  {} |\\n  null |\\n  undefined |\\n  {\'a\':\\n      { ... more optional params }\\n  }\\n```\\n\\nThe empty JSON object, `{}`, is tricky in JavaScript. There are several examples of testing for an empty object but they generally don\'t work as type guards in TypeScript for type safety.\\n\\n## Type guard with `in`\\n\\nAfter asking on [StackOverlow](https://stackoverflow.com/questions/75278401/how-do-i-get-type-target-history-from-github-graphql-on-defaultbranchref) and getting no response, I reached out to my local TypeScript expert for help.\\n\\nHe helped boil the issue down to the type shown in the previous code block with a type guard using the `in` keyword:\\n\\n```typescript\\nif (x !== null && x !== undefined && \\"a\\" in x) {\\n  // no null\\n  // not undefined\\n  // x has property \'a\' so it isn\'t empty\\n  console.log(x.a);\\n}\\n```"}]}')}}]);